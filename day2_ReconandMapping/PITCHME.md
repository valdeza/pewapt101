<!-- Automatically generated by Export-PptxToGitPITCHME.ps1
https://gist.github.com/valdeza/27845c35d14c833af63e449e5f055204 -->

<!-- Slide 1 -->
<!--[Rectangle 2]-->
Web Penetration Testing and Ethical Hacking  
Reconnaissance and Mapping

<!--[TextBox 4]-->
Copyright 2014, Secure Ideas
Version 1Q14

<!--[TextBox 2]-->
Professionally Evil   
Web Application 
Penetration Testing   
101.2

Note:
<!--[Rectangle 7]-->
1

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Web applications are a major point of vulnerability in organizations today. Web app holes have resulted in the theft of millions of credit cards, major financial and reputational damage for hundreds of enterprises, and even the compromise of thousands of browsing machines that visited web sites altered by attackers. 

In the next few days, you'll learn the art of exploiting web applications so you can find flaws in your enterprise's web apps before the bad guys do. Through detailed, hands-on exercises and these materials, you will be taught the four-step process for web application penetration testing. You will inject SQL into back-end databases, learning how attackers exfiltrate sensitive data. You will utilize Cross Site Scripting attacks to dominate a target infrastructure in our unique hands-on laboratory environment. And, you will explore various other web app vulnerabilities in-depth, with tried-and-true techniques for finding them using a structured testing regimen. As well as the vulnerabilities, you will learn the tools and methods of the attacker, so that you can be a powerful defender.

---

<!-- Slide 2 -->
<!--[Rectangle 2]-->
Course Outline

<!--[Rectangle 3]-->
- Day 1: Attacker's View, Pen-Testing and Scoping
- **Day 2: Recon & Mapping**
- Day 3: Server-Side Vulnerability Discovery
- Day 4: Client-Side Vulnerability Discovery
- Day 5: Exploitation
- Day 6: Capture the Flag

<!--[Right Arrow 3]-->

Note:
<!--[Rectangle 7]-->
2

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
In this class, we will learn the practical art of web application penetration testing. 

On Day 1, we will examine the attacker's perspective, and learn why it is important for us to build and deploy web application with the attacker's perspective in mind. We will also cover the pieces of a penetration test and how to scope and prepare for one.  Finally, we will explore the methodology that will be covered through the rest of class.
 
During Day 2, we will step through the process that successful attackers use to exploit applications, focusing specifically on the reconnaissance and mapping stages of the process.  This will give us the foundation we need to later control the application.

On Day 3, we will build upon that foundation and start discovering the various weaknesses within the applications.  As penetration testers, we will map out the attack vectors that we are going to use against this application. These discoveries will be the basis for the exploitation phase.

On Day 4, we will continue our discovery focusing on client side components such as Flash and Java.  We will also explore the client-side scripting in use within our applications.

On Day 5, we will launch the attacks that we planned and created during the previous three sections. We will also cover the next steps for students and where they should go from here.

On Day 6, we will be performing a web application pen-test within a capture the flag event.

---

<!-- Slide 3 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
- <span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
3

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Today we will cover the first two steps within the attack process.  Reconnaissance and mapping are the beginning steps and build the foundation of the attack.


---

<!-- Slide 4 -->
<!--[Rectangle 2]-->
Reconnaissance

<!--[Rectangle 3]-->
- Phase one of four in our web app penetration testing methodology
	- It guides our attacks as the test moves forward
- Often skipped because people believe they know all the information needed
- Easily the most critical step of the test
	- Important even for internal testing
- Results may be surprising, providing insight for improving security practices

Note:
<!--[Rectangle 7]-->
4

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
In this section of the class we are going to cover various topics that make up reconnaissance.  We will discuss identifying the infrastructure, machines and operating systems involved in the web application.  We will also look at profiling the servers and discovering the configuration of the various software pieces that make up the application.  Finally, we will look at some of the external sources of information available.

Reconnaissance is one of most important steps in the process of any attack.   Unfortunately, like breakfast, many people skip this step without ever realizing that it is vital.  

The powerful attacker, however, develops his or her strategy based on reconnaissance.  There are many tools  in your utility belt, and the appropriate combination can yield results others will have overlooked.  Reconnaissance allows us to craft our attack in an informed fashion, elevating our probability of success.




---

<!-- Slide 5 -->
<!--[Title 1]-->
# Recon Example


<!--[Content Placeholder 2]-->
- Recon search returns sample code posted by a developer for a target organization
	- Post to Google Groups
- At least two issues in this code snippet
	- SQL injection and XSS
- Valuable insight into their development practices
	- This code may exist in a publicly-accessible implementation!

<!--[Picture 3]-->

<!--[Left Arrow 4]-->

<!--[Left Arrow 5]-->

Note:
<!--[Rectangle 7]-->
5

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
As an example of the problems discoverable through recon, we were able to perform a simple search via Google Groups and find this discussion.  From the code that is shown, we are able to find two different potential vulnerabilities.  And this is without connecting to the target!

First we find where the developer is using a GET parameter in a database query without validation.  This is an obvious SQL injection flaw.  After this, we find that they are also displaying data directly from the database.  If the application allows for inserting data without encoding or filtering, this becomes a potential XSS issue.

By clicking on the "View Profile" link, we can then find both the company Michel is working for, and other posts by the developer.  These other posts may reveal even more issues with the target.

---

<!-- Slide 6 -->
<!--[Rectangle 2]-->
Target Selection:  
Defining the Test Scope

<!--[Rectangle 3]-->
- Testers need to know their targets
	- Could be target servers, indicated by individual IP address, range of addresses, and/or domain name(s)
	- Or, targets could be applications, on a single server or spread between multiple servers
- Target information can be gathered from:
	- Target system personnel
		- Usually in the Statement of Work and/or documentation they provide
	- Knowledge of the network
		- Internal teams usually know which applications exist
	- Processes
		- Change control procedures and software development processes are an excellent place for testers to determine which systems to test

Note:
<!--[Rectangle 7]-->
6

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
As a penetration tester, you will find there are several methods to selecting a targets:  provided by target system personnel, existing knowledge, and research.

Target system personnel will often provide a list of targets, to narrow the scope and allow for better use of your time (remember, your time is their money). This has several advantages and a drawback or two.  With reconnaissance time reduced, the work is more focused on mapping, discovery and exploitation.  However, testing can seem more contrived.  While good  scoping can protect you from being overwhelmed and running out of time, the natural attack pattern that reconnaissance can provide may fall outside the bounds of the test.  Be certain to discuss these challenges with clients and let them decide on the scope.  

An understanding of the target network is something which is situation-specific.  It is always good to lay out a scope, even if you are the one coming up with it.   Reconnaissance may assist in the scoping as well.  Don't be afraid to discuss the scope with the client, especially if you think it would be beneficial for the client to change it.

Black-box reconnaissance analysis is the most time-consuming but possibly the most real-life method for target selection. The benefit of this method is overcoming assumptions by administrators and developers who know the systems so well they overlook or miss threats and vulnerabilities.

---

<!-- Slide 7 -->
<!--[Rectangle 2]-->
Identifying Target Machines

<!--[Rectangle 3]-->
- Which machines are interesting and are part of the target application(s) and system(s)?
- Information gathered here will help guide our attacks
	- Vulnerabilities in the host
	- Which commands are supported by the host
- This includes infrastructure devices
	- Load balancers
	- SSL offload devices
	- Web App Firewalls
	- Proxies

Note:
<!--[Rectangle 7]-->
7

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Know your enemy. One of our first steps is to identify the machine, which means that we gather as much information as possible about the operating system, services, configuration, and relationship to surrounding systems. This information will guide our attack methodology. 

A big piece of the puzzle is the infrastructure that supports this application.  Some of the examples of this are load balancers and proxy servers.  One way to identify infrastructure pieces at this stage is through DNS records, though further testing later in the reconnaissance phase will reveal additional information.

---

<!-- Slide 8 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
- *** Whois and DNS***
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
8

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
The second step is to gather open source information about your target.  Remember that by open source we mean information that is publicly available, not something that Richard Stallman talks about.

---

<!-- Slide 9 -->
<!--[Rectangle 2]-->
Whois Records

<!--[Rectangle 3]-->
- Whois records identify the owner   
of a domain or IP address
- Domain whois records also   
include the authoritative name servers
- Used to determine other targets   
of interest
	- Other IP addresses owned by the target
	- Name Servers authoritative for the domain
- Query using web-based whois pages of registrars.
- Or use command-line whois tools

<!--[Picture 4]-->

<!--[Rectangle 7]-->
$ **whatis whois**
whois (1) - Internet domain name and network number directory service

Note:
<!--[Rectangle 7]-->
9

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
The WHOIS protocol has been used since the early 1980s to retrieve information about domain names and associated contact information. WHOIS can provide you with information such as a domain owner's name, contact information, and name servers which are authoritative for that domain. 

In the early days of the ARPANET there was only one central WHOIS server, but today there are many domain name registrars, and therefore many WHOIS servers. WHOIS servers for different domains return different information, and are maintained with varying levels of accuracy. Also, top-level domain registrars sometimes change their WHOIS server information without notifying the Internet Assigned Numbers Authority (IANA). Sometimes, if requested information is not available from one WHOIS server, the server will return a pointer to the appropriate registrar's WHOIS server. However, this is often not the case.  Ultimately it is up to your WHOIS client to query the correct WHOIS server.

WHOIS can provide you with a great deal of information, but remember that this information is only as accurate and up-to-date as the corresponding registrar requires. 


---

<!-- Slide 10 -->
<!--[Rectangle 2]-->
Whois Output

<!--[Content Placeholder 3]-->
- Records contain:
	- Contact information
		- E-mail addresses
		- Phone numbers
		- Names of technical and management staff
	- Authoritative name servers
		- As seen here, these may contain other domains related to the target

<!--[Picture 1]-->

Note:
<!--[Rectangle 7]-->
10

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
This slide shows a WHOIS response, shortened for display.

The "whois" tool is distributed with most Unix and Linux operating systems, and versions for Windows are available for download including the Microsoft Sysinternals tool at http://technet.microsoft.com/en-us/sysinternals/bb897435.aspx.  There are also many web-based WHOIS services including the service at Rusty Neddl's Ring of Saturn tool at http://networking.ringofsaturn.com/Tools/whois.php.

---

<!-- Slide 11 -->
<!--[Rectangle 15]-->

<!--[Straight Connector 10]-->

<!--[Title 1]-->
# Domain Name Services(DNS)


<!--[Content Placeholder 2]-->
- DNS is the phonebook of the Internet
- It maps friendly names to IP addresses, among other functions
	- www.secureideas.com to 52.88.70.156
- Each domain name has one or more authoritative servers
- These servers are responsible for the domain records
- All other servers should ask these for the information 

<!--[Straight Connector 10]-->

<!--[Text Box 13]-->
**Authoritative   
DNS Server**

<!--[Text Box 13]-->
**DNS   
Server**

<!--[Oval 75]-->
**2**

<!--[Text Box 13]-->
**Client's DNS server queries the authoritative name server, perhaps passing requests through other name servers recursively to get forwarded here**

<!--[TextBox 20]-->
**Client machines makes DNS request**

<!--[Picture 20]-->

<!--[Picture 21]-->

<!--[Picture 22]-->

<!--[Text Box 13]-->
**Browser**

<!--[Oval 75]-->
**1**

Note:
<!--[Rectangle 7]-->
11

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
DNS is the phone book of the Internet, mapping human-readable names to IP addresses.  When a client attempts to resolve a hostname to an IP address, it will query the local DNS server, which will ultimately query the authoritative names server(s) for the target domain.

Authoritative name server can provide a wealth of useful information for a penetration tester, when prompted with specially-formatted queries.

---

<!-- Slide 12 -->
<!--[Rectangle 2]-->
Nslookup

<!--[Rectangle 3]-->
- DNS query tool
- Nslookup will query our  
local DNS server unless  
directed otherwise
	$ **nslookup [host] [DNS_server]**
- nslookup uses an interactive mode if no host is specified
- **set debug **instructs nslookup to return more information
- On Windows, it is the preferred built-in tool for DNS lookup
- On Linux/Unix, nslookup is deprecated for dig
	- It's still there in most distributions, but it's not the preferred solution
	- It's functionality has been scaled back (no zone transfers in recent versions)

<!--[Picture 1]-->

Note:
<!--[Rectangle 7]-->
12

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
To reduce the load on the high-level name servers, the DNS protocol allows for caching. Each DNS record includes a "time to live" value, which specifies the maximum amount of time that a DNS resolver can cache the record. 

When you type "www.secureideas.com" into your browser, your computer will first check to see if the address is cached by your browser. If not, it will check your operating system's local cache, and then it will check your ISP or organization's DNS server. If the information is not cached there, your ISP or organization's DNS server will recursively query the global hierarchy of nameservers and return the result to your host. 

The DNS service typically listens on UDP port 53. It is also used for a variety of other functions, such as mapping mail exchangers to domains.

The "nslookup" tool is used to perform DNS queries from the command line. It is available on Unix, Linux and Windows. 

There are also many web sites which offer an "nslookup" service including the Zoneedit service at www.zoneedit.com/lookup.html.  Attackers often prefer web-based nslookup because the target will only have a record of the web site's address, not the attacker's originating IP address.

The Linux nslookup tool has been depreciated in favor of "dig" tool, but it is still the preferred tool for Windows systems and will be used throughout this class.

---

<!-- Slide 13 -->
<!--[Rectangle 2]-->
Dig

<!--[Rectangle 3]-->
- Included with BIND tools
	- Available for Windows through Cygwin
- Dig will search for specific types of records using **-t**
	- **-t MX **for mail servers
	- **-t AXFR **for a zone transfer
	- **-t ANY** for any records
- Attempt to capture all DNS records with a zone transfer on all authoritative DNS servers 
	- Don't forget upstream ISP name servers 

<!--[Picture 1]-->

Note:
<!--[Rectangle 7]-->
13

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Domain Information Groper ("DIG" or "dig") is a modern replacement for the nslookup tool.  It is used for conducting DNS queries. Although written for use on Unix/Linux systems, dig can be used on Windows through Cygwin, a Unix environment that runs on Windows systems, available at www.cygwin.com.

Here is a simple example using dig:

$ dig -t ANY www.secureideas.com

; <<>> DiG 9.9.5-3ubuntu0.15-Ubuntu <<>> -t ANY www.secureideas.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 27769
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 512
;; QUESTION SECTION:
;www.secureideas.com.           IN      ANY

;; ANSWER SECTION:
www.secureideas.com.    3600    IN      A       52.88.70.156

;; Query time: 163 msec
;; SERVER: 75.75.75.75#53(75.75.75.75)
;; WHEN: Sun Jan 28 22:04:47 STD 2018
;; MSG SIZE  rcvd: 64

---

<!-- Slide 14 -->
<!--[Rectangle 2]-->
Fierce Domain Scanner

<!--[Rectangle 3]-->
- Simple scanner designed to find hosts  
within a domain
- Written by Robert "RSnake" Hansen and   
updated to 2.0 by Joshua "Jabra" Abraham
- Performs a series of tests to find hosts
	- Queries the system DNS servers to find  
target DNS servers
	- Attempts to retrieve SOA records
	- Proceeds to guess host names using a wordlist
	- Once an IP address is found, does a reverse lookup on next 5 and previous 5 IP addresses
	- Uses a set of prefixes to find hosts (i.e. www2, www3)
- Supported on most platforms as it is written in Perl

<!--[Picture 4]-->

Note:
<!--[Rectangle 7]-->
14

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
The Fierce domain scanner is a tool written by RSnake to find hosts within a network.  It is not a IP scanner like Nmap but it is a great precursor to port scanning. It uses DNS to find hosts by performing forward and reverse lookups against a domain's authoritative name servers.  It then proceeds to scan using various methods that depend on how the command is invoked.  It is written in Perl and runs under most Perl implementations, though it has not been tested using ActivePerl.

Lookup various hosts in a domain
	perl fierce.pl -dns secureideas.com
Scan an IP range
	perl fierce.pl -range 192.168.1.0-255 -dnsserver ns1.secureideas.com 
Perform reverse lookups after finding CNAMEs
	perl fierce.pl -dns secureideas.com -wide -output output.txt 

The Fierce Domain Scanner is freely available at http://ha.ckers.org/fierce/.

---

<!-- Slide 15 -->
<!--[Title 1]-->
# DNSrecon


<!--[Content Placeholder 2]-->
- Performs DNS enumeration on a target domain
	- Collects standard records such as A, NS, SOA and MX
	- Attempts zone transfers
	- Whois queries for the IP addresses discovered
	- Reverse lookup for a given range of IP addresses
- Written by Carlos "darkoperator" Perez
- Python script

Note:
<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
DNSrecon was written by Carlos "darkoperator" Perez to enumerate DNS information and gather data associated with a targeted domain.  The script takes a number of the reconnaissance tasks that we've discussed and centralizes them into a single script.  In particular, it collects any SRV records that are configured for a domain.  This can be useful for finding additional services that could be in scope for our testing.  DNSrecon is available at https://github.com/darkoperator/dnsrecon.

<!-- SKIPPED: [Slide Number Placeholder 3]-->

---

<!-- Slide 16 -->
<!--[Title 1]-->
# DNSrecon output


<!--[Picture 3]-->

Note:
<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
The output from DNSrecon is straightforward to read.  In this example, we can see some basic information about hosts, SPF and TXT records within secureideas.com.  Note that the script will give us informtion about DNSSEC, SRV records and can perform whois queries for the IP addresses discovered.  We can also decide whether to perform reverse DNS lookups on each IP in the ranges reported by the whois information.

<!-- SKIPPED: [Slide Number Placeholder 3]-->

---

<!-- Slide 17 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		- *** Exercise: DNS Harvesting***
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
17

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
In this exercise we will work with the various DNS tools covered in the last few slides.

---

<!-- Slide 18 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
- *** External Information Sources***
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
18

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
The second step is to gather open source information about your target.  Remember that by open source we mean information that is publicly available, not something that Richard Stallman talks about.

---

<!-- Slide 19 -->
<!--[Rectangle 2]-->
Open Source Information

<!--[Rectangle 3]-->
- Open source information is publically available
	- "Open source" in this context doesn't mean the Linux source code ?
- The information gathered may be sensitive
- The tester needs to examine the data as a whole to see what is revealed
- This information can be gathered without ever connecting to target

<!--[Picture 1]-->

<!--[Picture 6]-->

<!--[Picture 4]-->

<!--[Picture 5]-->

Note:
<!--[Rectangle 7]-->
19

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
A great wealth of information can be obtained without ever touching the targets or alerting them to your intentions.  This is accomplished by searching third-party databases such as WHOIS, DNS, caching sites and various search  engines.  

Press releases are frequently available electronically and easily found using a search engine.

Newsgroups can be great sources of technical information about a target, as many administrators will post to newsgroups and mailing-lists without considering the retention period of such posts nor the value of the information they reveal.

Social Networks such as FaceBook, Linked-In, and MySpace tend to provide a great deal of information about individuals, particularly personal information which may be leveraged in many different attacks.

Search engines can be any online webcrawler including MSN, Yahoo, Altavista, and of course Google.  **All** sorts of valuable tidbits can be found, relating to basically any website on the Internet.

---

<!-- Slide 20 -->
<!--[Rectangle 2]-->
Search Engines

<!--[Rectangle 3]-->
- Search engines are the best source of data around
- The technique of finding vulnerabilities or other sensitive information from search engines is commonly called "Google Hacking"
	- Even though other search engines are used to provide a wider view
- Amazingly effective due to the amounts of information people reveal purposely or inadvertently

<!--[Picture 5]-->

<!--[Picture 6]-->

<!--[Picture 4]-->

<!--[Picture 7]-->

Note:
<!--[Rectangle 7]-->
20

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
"Google Hacking" is an exceptionally powerful tool for reconnaissance.  Google indexes and maps websites the world over, storing a copy of the HTML for each page found.  This wealth of information coupled with a powerful search interface makes "Google Hacking" a vital part of reconnaissance analysis.

Please note that "Google Hacking", despite the name, is not limited to Google.   Indeed, several other search engines are also used, some offering distinct advantages over Google.  Because Google has been particularly useful for identifying technical content, IT professionals have a natural affinity to use Google over other search engines.  The Google Hacking concepts, however, apply to all search engines.

---

<!-- Slide 21 -->
<!--[Rectangle 2]-->
Search Engine Directives

<!--[Rectangle 3]-->
- Search engines support various search directives and operators
- Directives limit search results, letting you focus your search
- These are from Google
	- **site:www.secureideas.com**
	- **inurl:phpinfo**
	- **intitle:"Admin Login"**
	- **link:secureideas.com**
	- **ext:xls**
- Bing supports
	- **site:www.secureideas.com**
	- **inanchor:phpinfo**
	- **intitle:"Admin Login"**
	- **filetype:xls**

Note:
<!--[Rectangle 7]-->
21

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Search engines provide advanced search directives that allow users to narrow their search results, or to focus their analysis based on multiple criteria.  These directives are also very useful in a penetration test for reconnaissance analysis.

Google supports the following within their modifiers:

*site* - The *site *directive allows the user to limit the results to a target site or domain.  If you have one or more specific sites or domains within the scope of your analysis, this directive allows you to focus any search results to your target site.

inurl - The *inurl *directive allows the user to search for keywords within the URL of a page.  This directive is often useful for identifying pages, scripts or vulnerable executables that appear in the URL bar, as opposed to as just a string on a page.

intitle - The *intitle *directive allows the user to search for keywords within the title of a page (e.g. the content within <title> and </title>).  This is useful for narrowing down search results to a page that includes a keyword within the title and, presumably, would have a significant focus in the page for that keyword as opposed to a common string match somewhere within the page.  Tomorrow, we'll look at searches that include the title "Index Of" to identify sites that have left directory browsing enabled.

link - The *link *directive allows the user to identify sites that link to our target, providing information that is useful for social engineering and related attacks including user name harvesting based on knowledge of business partners.

ext - The *ext *directive allows the user to search for files with an identifiable extension, such as Excel spreadsheets shown in the example on this slide.

Bing also supports the site:, intitle:, and the filetype:,  For the inurl:, Bing uses inanchor:.

---

<!-- Slide 22 -->
<!--[Rectangle 2]-->
Modifiers to  
Focus Searches

<!--[Rectangle 3]-->
- Surround strings with double quotes for literal matches: ""
	- **"Secure Ideas Web Application"**
	- This also forces the search engine to include all of the words
- The "-" omits pages or pages with specific strings from results
	- **site:secureideas.com -site:www.secureideas.com**
	- **site:secureideas.com -handlers**
	- Bing uses** Not** instead
- "*" is used as keyword wildcard

Note:
<!--[Rectangle 7]-->
22

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Often, Google searches return many more results than we are interested in evaluating one-by-one.  Fortunately, Google includes search modifiers which are useful for limiting the scope of your search, allowing you to make the most effective use of your limited reconnaissance time.

Searching for quoted strings (e.g. "search keywords") searches for the specified string, not simply the words that make it up.  The "-"  (hyphen, or minus sign)  can also be used to remove keywords words from the search.   This is very helpful for narrowing the results of your search.  For example, when searching for Kevin Johnson, the SANS instructor but not the famous NBA star nor the ventriloquist, we can use the syntax "-athlete -nba -ventriloquist" to narrow our search results. Use caution with the hyphen modifier, however, since it is easy to exclude pages which would legitimately show the content you are interested in finding.

The "*" (star, or asterisks) character is used as a keyword wildcard.  This modifier is useful in quoted search strings, substituting a star for any keyword in your search to open your search up for keywords you might not know at the beginning of your analysis.

---

<!-- Slide 23 -->
<!--[Rectangle 2]-->
Google Hacking

<!--[Rectangle 3]-->
- The Google Hacking Database (GHDB) is a source of queries to find interesting information (Google Dorks)
- Database entries include searches for files containing passwords, vulnerable applications, useful error messages
- Works on most search engines with some syntax changes
- Now at http://www.exploit-db.com/google-dorks/

<!--[Picture 3]-->

Note:
<!--[Rectangle 7]-->
23

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Johnny Long, world-famous for his book "Google Hacking for Penetration Testers," maintains the Google Hacking Database (GHDB)  at http://johnny.ihackstuff.com/.  The GHDB is a repository for search syntax (known as "Google Dorks") which can reveal useful information for a hacker.  As a user-contributed site, the GHDB is constantly expanding with new searches for identifying files containing passwords or sensitive information, unprotected HTTP webcams, vulnerable web applications or simply identifying useful error messages that reveal information about a target.

While the searches in the GHDB are formatted for use with Google, they can also be adapted to work with other search engines after minor syntax changes.  This is especially advantageous when Google decides to blacklist some of the GHDB searches that appear to be hacking attempts; other search engines seldom blacklist their search results in this fashion.

It is now available at http://www.exploit-db.com/google-dorks/

In the next few slides we'll see some automated Google Hacking tools, most of which leverage the GHDB entries.

---

<!-- Slide 24 -->
<!--[Rectangle 2]-->
Automating Google Searches

<!--[Rectangle 3]-->
- A number of tools are available to automate Google searches
	- Some require a Google SOAP API key
- Google stopped issuing new SOAP API keys in December 2006
- In response, SensePost released SPUD
	- Tool that converts Google SOAP API request into general searches of the Google website
- Great, except:
	- Violates Google's terms of service
	- Could get you shunned from Google temporarily

Note:
<!--[Rectangle 7]-->
24

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Why don't we just script up a tool which automatically searches Google for  information of interest in penetration testing?  GREAT IDEA!  In fact, others have already created powerful tools to do just that, originally around the Google Simple Object Access Protocol (SOAP) API and Google API key.  Unfortunately, the Google SOAP functionality has been deprecated, and Google no longer provides the keys necessary to use this API.  In response the SensePost information security group released the Aura tool.

Aura provides an interface that replicates the once-available Google SOAP API, turning SOAP requests into standard Google search queries and returning the results.  Instead of doing this through the Google API (which would require a Google API key), Aura uses "screen-scraping" to collect, parse and return the results.  This effectively allows applications based on the Google SOAP API to function without a Google API key.  Unfortunately, the screen-scraping method used by Aura for collecting Google search results is a violation of Google's terms of service.  According to the terms of service, Google has the right to shun users from all searches if the terms are violated.

---

<!-- Slide 25 -->
<!--[Title 1]-->
# Google Alerts


<!--[Content Placeholder 2]-->
- Automated search method showing new results based on any search terms
- Notification through e-mail, daily or weekly
- Using a Google account allows you to manage Google alerts
- Primarily useful for internal teams due to its scheduled nature

<!--[TextBox 3]-->
Search terms go here

<!--[Rectangle 11]-->
Useful Google Alerts During a Penetration Test

<!--[Picture 1]-->

<!--[Content Placeholder 2]-->
- New pages as the test progresses:  
site:secureideas.com
- New site error messages:  
site:secureideas.com intitle:"Error Occurred While"
- New pages with sensitive information  
site:secureideas.com "Index of /" +passwd

<!--[Left Arrow 4]-->

Note:
<!--[Rectangle 7]-->
25

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
The Google Alerts system is an automated search method built into the Google web site.  Users can establish any search criteria and retrieve results through e-mail, delivered daily, weekly or as it happens.  Using Google Alerts, users can set up quick searches or, using a Google account, set up and manage long term automated searches.

Typically Google Alerts provides more functionality to internal test teams as the system is geared toward scheduled searching.  Some of the things we could use it for are to find new pages on the site or look for leaked information such as credit card numbers or user listings.

---

<!-- Slide 26 -->
<!--[Rectangle 2]-->
Newsgroups and Mailing Lists

<!--[Rectangle 3]-->
- Newsgroups and mailing lists contain tons of information
- People treat them as private sources of help or stress release
	- Requests for help using a technology
	- Code snippets with questions for how to make it all work
	- Rants and tirades from employees
	- Discussions about technology
- Newsgroups via Google Groups
- Mailing list archives available in many sources
	- Usually indexed by Google

<!--[Picture 3]-->

<!--[Picture 4]-->

<!--[Rounded Rectangle 6]-->

<!--[Rounded Rectangle 6]-->

<!--[TextBox 3]-->
Running a query with the input

<!--[Left Arrow 4]-->

<!--[TextBox 3]-->
Accepting user input without filtering

<!--[Left Arrow 4]-->

Note:
<!--[Rectangle 7]-->
26

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Newsgroups and mailing lists have been a bastion of information, for the posters and attackers alike.  Sometimes you will see configuration files for network gear or other equipment posted to discussion groups (at times the password hashes aren't even sanitized!)  You may also find web site architecture information, or all kinds of other juicy tidbits.  

Several years ago Google purchased DejaNews and turned it into Google Groups.   You can either visit groups.google.com, or run a Google search and select "Groups" from the top menu bar (at the time of this writing, "Groups" is under the "More" drop-down menu).  Searching in this fashion allows you to  review both mailing list archives and news groups at once. 

The screenshots shown are from a simple query looking for the string **qry **and **php **which returns many hits of PHP developers showing snippets of their code.  This example is a typical SQL injection flaw.

---

<!-- Slide 27 -->
<!--[Rectangle 2]-->
Google Groups

<!--[Rectangle 3]-->
- Google maintains a huge archive of thousands of newsgroups at groups.google.com
- Searches here use the same modifiers as general Google searches
- Additionally, Google Groups supports special groups-specific directives
	- **author:kjohnson@secureideas.com**
	- **author:@secureideas.net**
	- **insubject:"Problems with my code"**

Note:
<!--[Rectangle 7]-->
27

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Google Groups is another Google search site, indexing the content of Usenet news group postings.  Fortunately, Google Groups supports the same search directives used for standard Google search directives.  In addition to the standard search directives, Google Groups also takes advantage of the From and Subject headers in newsgroup messages to introduce two new search directives: author and insubject.

*author *- The *author* directive allows the user to search the Google Groups database for author information, using an e-mail address or the reported name of the person posting the article.

*insubject *- The *insubject *directive searches the subject line of the newsgroup posting for the specified keyword(s).**


---

<!-- Slide 28 -->
<!--[Rectangle 2]-->
Social Networks

<!--[Rectangle 3]-->
- Social networks are one of the more popular destinations on the web
- Many sites allow for searching based on company name
	- Some even have groups created by employees
- Lots of information is disclosed on these sites
	- Personal data for social engineering attacks
	- Answers to password reset questions
	- Data regarding technologies used by the target
- Recommend investigating Facebook, LinkedIn, Twitter, MySpace

<!--[Picture 3]-->

<!--[Picture 6]-->

<!--[Picture 1]-->

Note:
<!--[Rectangle 7]-->
28

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Reading someone's social networking page(s) is often like reading a virtual diary.  All sorts of personal information can be harvested which may prove beneficial in any number of attacks (for example, guessing password-reset questions).  While not specifically web application-focused, harvesting this type of information is vital for any penetration test.

Here are some examples of various social network pages.  We have Kevin's Facebook and LinkedIn profiles, as well as James Jardine's Twitter account. This type of information can be very useful in attacking web applications and social engineering attacks.

---

<!-- Slide 29 -->
<!--[Title 1]-->
# Automated Social Network Parsing


<!--[Content Placeholder 2]-->
- Let's try out a tool  that finds target data in social networks
- Jason Wood created the Reconnoiter to pull data from LinkedIn
	- Uses the names in LinkedIn profiles from target employees to generate potential login names
	- Also collects URLs to Linkedin profiles for targeted organizations
	- Available at http://sourceforge.net/projects/reconnoiter/

Note:
<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
While it is possible to grab all of this data manually, it makes sense to use tools that can help.  One set of scripts that is currently available is from Jason Wood. The first script is actually two different scripts.  They both use Linkedin to find target employees.  They then generate a list of potential user names from that list. The difference between the two scripts is that one of them uses Google and the other uses Yahoo.  The second script performs a similar search and prints out the target employees' name and the URL of their Linkedin profile.

<!-- SKIPPED: [Slide Number Placeholder 3]-->

---

<!-- Slide 30 -->
<!--[Picture 3]-->

<!--[Title 1]-->
# Using the Scripts


<!--[Text Box 51]-->
**Using the Linkedin profile harvester to collect Secure Ideas profiles**

<!--[Text Box 51]-->
**Keep in mind some false positive will happen:**
****
**John Overbaugh in this example**

Note:
<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
As we can see in the above screen shots, the scripts are pretty simple to run.  The scripts are given the target name and the number of pages of results it should parse.

Keep in mind that the target organization name can cause issues.  The scripts accept the name of the organization, but do not know the different ways people could reference them.  For example, a lot of organizations use abbreviations or secondary names.  The scripts cannot find these.

<!-- SKIPPED: [Slide Number Placeholder 3]-->

---

<!-- Slide 31 -->
<!--[Title 1]-->
# theHarvester


<!--[Content Placeholder 2]-->
- theHarvester gathers information from target domains via public information sources
	- Email addresses
	- IP addresses and domain names
	- Ports and banners
- Uses search engines, PGP key servers and SHODAN
- Written by Christian Martorella in Python

<!--[Picture 3]-->

Note:
<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
theHarvester is automates the collection of email addresses, IP addresses, domain names and other information by using search engines, PGP key servers and the SHODAN database.  It is able to pull results from search engines via screen scraping or API calls.  It should be handled with care as this script can also cause your IP address to be shunned by the search engines.

In this example, theHarvester is pulling information about secureideas.com from Bing.  The information available will change quite a bit between the different information sources, so plan on running your target domain through several of them.

<!-- SKIPPED: [Slide Number Placeholder 3]-->

---

<!-- Slide 32 -->
<!--[Rectangle 2]-->
Maltego

<!--[Rectangle 3]-->
- Maltego is an information mapping tool that finds the relationships between people, sites, and companies
- Uses "transforms" to build a hierarchy of related information from a starting point
	- Domain name, a person's name, a phone number, etc.
- Some interesting transforms:
	- Domain to PGP Keys
	- Person to e-mail
	- Domain to phone numbers
- Two editions available: Community and Professional
	- Free version that has certain limitations
	- Fully featured tool for professional reconnaissance

<!--[Picture 4]-->

Note:
<!--[Rectangle 7]-->
32

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Maltego is an open source intelligence and forensics application written by Paterva.  It automates the information mining that we have been discussing in this module, querying information from multiple sites and evaluating resources to collect data based on selected "transforms".  Maltego provides multiple useful transforms to collect information that is desirable, such as using a domain name to retrieve all published PGP keys, or using a person's name to identify their e-mail address, or using a domain name and retrieving all phone numbers associated with that domain.

While the information that is accessible through Maltego can be retrieved from publicly-accessible sources, Maltego makes it tremendously easier to mine valuable information from a specified target, saving the pen-tester from tedious searches and data analysis tasks.  Maltego also provides a terrific interface for navigating and displaying the results of information, which provides insight into the relationship between a target and other resources (such as a target site and the web hosting provider, for example) that could easily be missed when assessing the results from a standard search engine.

Maltego comes in two editions; an unrestricted professional edition, and a community edition that provides some functionality with significant limitations.  I recommend highly that if you are going to use Maltego, purchase the professional version as the community edition is really similar to trial software.  Both the community and professional editions of Maltego are available at http://www.paterva.com/maltego/.



---

<!-- Slide 33 -->
<!--[Rectangle 2]-->
Differences Between Maltego  
Community Edition and Pro

<!--[Rectangle 3]-->
- Only returns 12 results per transform
- Limited zoom levels
- Can only run transforms on a single entity at a time
- Cannot copy and paste text from detailed view
- Limits us to 75 transforms a day
	- These transforms are the searches that do the work building the map
- Communication between client & server is not encrypted
- Throttles the communication between Maltego and the server that performs the transforms

Note:
<!--[Rectangle 7]-->
33

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
As explained earlier, the community edition has a number of limitations. It limits the amount you can zoom in.  It also can only run transforms, the mining, on a single entity and when viewing the detailed information, you cannot cut and paste.  The system also limits you to 75 transforms a day and throttles the communication to the transform server.

---

<!-- Slide 34 -->
<!--[Rectangle 2]-->
Recon-ng

<!--[Rectangle 3]-->
- Recon-ng is a web reconnaissance framework by Tim Tomes (LaNMaSteR53)
	- https://bitbucket.org/LaNMaSteR53/recon-ng
- Written in Python
- Interface modeled after Metasploit
- It includes many modules:
	- Dozens of Recon modules interact with Internet services to obtain info
	- Reporting modules consolidate and export results (e.g. to csv)
	- A few Discovery and Exploitation modules
- Many modules interact with web interfaces
	- "Sleep" implemented to avoid shunning
- Other modules require API key
	- Some keys are free
	- Most are not

<!--[Picture 3]-->

Note:
<!--[Rectangle 7]-->
34

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Recon-ng is a great tool for automating many common recon tasks.  The text-based interface is modeled after Metasploit, and involves navigating into modules, setting options, and running the module.  Many of the modules have a "web-version" that can query the web interface, but these are implemented with a "sleep" between requests to avoid shunning.  The API-based modules tend to work much faster but require API keys.  Some API keys are free but many have a cost.

---

<!-- Slide 35 -->
<!--[Rectangle 2]-->
Recon-ng:  
Important Commands

<!--[Rectangle 3]-->
- "help": lists all available commands
- "show": retrieve information from the framework such as:
	- "show modules": lists all modules
	- "show options": lists option settings globally if at root or for a module if in a module.
	- "show dashboard": summarize results for current workspace
	- "show workspaces": list available workspaces
- "use" or "load": navigate into the context of the specified module
- "set": set an option to the specified value
- "keys": manage API keys
- "info": display information about the current module
- "run": execute the current module
- "back": go back one navigation level.  Backing all the way out will exit recon-ng.

Note:
<!--[Rectangle 7]-->
35

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Recon-ng commands are similar to those found in Metasploit.  The list here includes some of the most important commands for navigating through Recon-ng and executing a module.  The most important command is the first one (help), as Recon-ng's embedded help system provides directions for just about every aspect of using the framework.

---

<!-- Slide 36 -->
<!--[Rectangle 2]-->
Recon-ng:  
Workspaces

<!--[Rectangle 3]-->
- Keep information about different targets organized
- Most Global options can be set per-workspace
- Display available workspaces with "show workspaces"
- Switch to a workspace with "set WORKSPACE <workspacename>"
	- If the specified workspace does not exist, a new one will be created

<!--[Picture 1]-->

Note:
<!--[Rectangle 7]-->
36

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Recon-ng commands are similar to those found in Metasploit.  The list here includes some of the most important commands for navigating through Recon-ng and executing a module.  The most important command is the first one (help), as Recon-ng's embedded help system provides directions for just about every aspect of using the framework.

---

<!-- Slide 37 -->
<!--[Rectangle 2]-->
Recon-ng:  
Types of Recon Modules

<!--[Rectangle 3]-->
- Modules to gather contacts associated with target
	- recon/contacts/
- Modules to gather credentials
	- recon/creds
	- All of these require API keys
- Modules to gather host information
	- recon/hosts/gather for finding hosts and host details
- Modules to gather geo-location information related to the target
	- recon/pushpin
	- recon/hosts/geo for finding physical location specific to hosts

Note:
<!--[Rectangle 7]-->
37

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Recon-ng comes equipped with a growing number of recon modules designed to gather information about a target without actually touching the target.  There are modules to gather contacts, credentials, host, and geo-location information.  The credential information may be some of the most interesting but it is also the most costly and requires API keys.

---

<!-- Slide 38 -->
<!--[Rectangle 2]-->
Recon-ng:  
API Keys

<!--[Rectangle 3]-->
- Many modules require API keys to work
- Reference the wiki for instructions on obtaining keys: https://bitbucket.org/LaNMaSteR53/recon-ng/wiki/Home
- To install a key:
	- "keys add <name> <key>"
- Use "keys list" to see current list of installed API keys
- Use "keys" for additional commands
	- 

<!--[Picture 2]-->

Note:
<!--[Rectangle 7]-->
38

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
This is a simple example for gathering contacts from a target (Secure Ideas), using the jigsaw web module.  This one does not require an API key.

---

<!-- Slide 39 -->
<!--[Rectangle 2]-->
Recon-ng:  
Example 1: Contacts

<!--[Rectangle 3]-->
- Use the Jigsaw web Module to gather some contacts.
	- use recon/contacts/gather/http/web/jigsaw
	- set COMPANY Secure Ideas
	- run
	- 

<!--[Picture 1]-->

Note:
<!--[Rectangle 7]-->
39

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
This is a simple example for gathering contacts from a target (Secure Ideas), using the jigsaw web module.  This one does not require an API key.

---

<!-- Slide 40 -->
<!--[Rectangle 2]-->
Recon-ng:  
Example 2: Hosts

<!--[Rectangle 3]-->
- Use the Google module to find target sites:
	- use recon/hosts/gather/http/web/google_site
	- set DOMAIN secureideas.com
	- run
- Use the DNS resolve module to populate ip addresses
	- use recon/hosts/enum/dns/resolve
	- run
- Type "show hosts" to see the results table
	- 
	- 

<!--[Picture 3]-->

Note:
<!--[Rectangle 7]-->
40

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
This is a simple example for gathering contacts from a target (Secure Ideas), using the jigsaw web module.  This one does not require an API key.

---

<!-- Slide 41 -->
<!--[Rectangle 2]-->
Recon-ng:  
Sample Modules for Contacts

<!--[Rectangle 3]-->
- recon/contacts/enum/http/web/dev_diver
	- This module takes a username and searches common, public code repositories for information about that username.
- recon/contacts/enum/http/web/namechk
	- Leverages NameChk.com to validate the existance of usernames at specific web sites.
- recon/contacts/enum/http/web/pwnedlist
	- Leverages PwnedList.com to determine if email addresses are associated with leaked credentials and updates the 'creds' table of the database with the positive results.
- recon/contacts/gather/http/api/linkedin_auth
	- Harvests contacts from the LinkedIn.com API using an authenticated connections network and updates the 'contacts' table of the database with the results.
- recon/contacts/gather/http/web/jigsaw
	- Harvests contacts from Jigsaw.com and updates the 'contacts' table of the database with the results.
- recon/contacts/gather/http/web/pgp_search
	- Searches pgp.rediris for email addresses for the given domain.

Note:
<!--[Rectangle 7]-->
41

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
The first one (dev_diver) will look for username comments in public code repositories, which is helpful in identifying vulnerabilities that a developer may have posted somewhere.  LinkedIn requires an API key, which can be had for free.  Jigsaw is somewhat general purpose, as the contacts can be any role.  The pgp_search modules is very interesting when you think about who tends to use pgp keys (i.e. usually tech-savvy people with something to hide).

---

<!-- Slide 42 -->
<!--[Rectangle 2]-->
Recon-ng:  
Modules for Creds

<!--[Rectangle 3]-->
- The modules that can be used to gather creds are largely tied to:
	- Pwnedlist
	- Leakdb
	- Noisette
- Most of these require API keys that come at a cost
- For some companies, the cost is worthwhile
	- E.g. to produce a list of employees who's credentials are known to be compromised

Note:
<!--[Rectangle 7]-->
42

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
For many web pen testers, the cost of the API keys necessary for most of the Creds modules may be out of reach.  Still, these can provide a tremendous value in certain cases - especially if, for example, a company wishes to produce a list of employees who need to go change their passwords immediately because they are known to have been compromised.

---

<!-- Slide 43 -->
<!--[Rectangle 2]-->
Recon-ng:  
Sample Modules for Hosts

<!--[Rectangle 3]-->
- recon/hosts/enum/dns/resolve
	- Resolves the IP addresses for the hosts from the 'hosts' table of the database and updates the 'hosts' table with the results.
- recon/hosts/enum/http/api/whois_lookup
	- Uses the ARIN Whois RWS to query whois data for the given IP addresses.
- recon/hosts/enum/http/web/netcraft_history
	- Checks Netcraft.com for the hosting history of the given target(s).
- recon/hosts/enum/http/web/xssed
	- Checks XSSed.com for XSS records for the given domain and displays the first 20 results.
- recon/hosts/gather/http/api/bing_ip
	- Leverages the Bing API and "ip:" advanced search operator to enumerate other virtual hosts sharing the same IP address.
- recon/hosts/gather/http/api/shodan_hostname
	- Harvests hosts from the Shodanhq.com API by using the 'hostname' search operator and updates the 'hosts' table of the database with the results.
- recon/hosts/support/add_host
	- Manually adds a host.

Note:
<!--[Rectangle 7]-->
43

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
There are more host modules than any other type because of the large number of host-related services available on the Internet.  This is a small sample of some of the more interesting hosts modules.  However, web pen testers using recon-ng should peruse the entire list.  Note that any module with /api/ in its path name requires an API key.  Most of these have instructions on the Recon-ng Wiki.

---

<!-- Slide 44 -->
<!--[Rectangle 2]-->
Recon-ng:  
Sample Modules for Geo-Location

<!--[Rectangle 3]-->
- recon/hosts/geo/http/api/hostip
	- Leverages the hostip.info API to geolocate the given host(s) by IP address and updates the 'hosts' table of the database with the results.
- recon/hosts/geo/http/api/ipinfodb
	- Leverages the ipinfodb.com API to geolocate the given host(s) by IP address and updates the 'hosts' table of the database with the results.
- recon/pushpin/picasa
	- Searches Picasa for media in specified proximity to the given location.
- recon/pushpin/shodan
	- Searches Shodan for hosts in specified proximity to the given location.
- recon/pushpin/twitter
	- Searches Twitter for media in specified proximity to the given location.

Note:
<!--[Rectangle 7]-->
44

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Geo location is an interesting and relatively new area of recon.  There are a few modules that can help with identifying servers, people, photos, and videos within a particular area.  This type of information is invaluable when planning a physical or social engineering test.  Surveillance data can be gathered without any real trace or physical presence.  A picture including employees can aid a tester in duplicating an employee ID badge for use in a physical pen test.

---

<!-- Slide 45 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
- *** Mapping***
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
45

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Mapping is the second step in the attack process.  We have completed reconnaissance and now are moving up the process stack. In mapping, we are going to cover various different pieces.  First spidering or downloading the entire site.  Then we will chart the application flow and analysis the relationships between pages.  During all of this we will be gathering session tokens for analysis later.

The next step is to identify any machines that are used within the application and are visible to the client.  Keep in mind that visible does not mean readily apparent from the client browser.

---

<!-- Slide 46 -->
<!--[Title 1]-->
# Mapping Phase Components


<!--[Content Placeholder 2]-->
- The mapping phase consists of several components
- They are generally followed in this order, although pragmatism is important
- Keep in mind that we may iterate through steps as we find more data

<!--[Rectangle 19]-->
Port Scan

<!--[Rectangle 20]-->
OS Fingerprint & Version Scan

<!--[Rectangle 21]-->
SSL Analysis

<!--[Rectangle 22]-->
Virtual Hosting & Load Balancer Analysis

<!--[Rectangle 23]-->
Software Configuration Analysis

<!--[Rectangle 24]-->
Spidering

<!--[Rectangle 25]-->
Detailed Analysis of Spidering Results

<!--[Curved Connector 11]-->

<!--[Freeform 12]-->

<!--[Freeform 13]-->

<!--[Freeform 14]-->

<!--[Freeform 15]-->

<!--[Freeform 16]-->

<!--[Freeform 17]-->

<!--[Freeform 18]-->

Note:
<!--[Rectangle 7]-->
46

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
The mapping phase can be broken into multiple different components.  These items, such as port scanning and spidering, are generally performed in the order above, but keep in mind that we can make changes in the order as the environment and test warrant.  For example, performing the OS fingerprinting and version scan at the same time as the port scan makes sense since it is typically part of the same tool.  

We will iterate through these steps during a test.  One reason for this would be that as we spider a site, we may find new target servers that need to be evaluated.  Once we find these servers, we will need to perform these steps against that machine.

---

<!-- Slide 47 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
- *** Port Scanning, OS Finger-  
     printing, & Version Scanning***
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
47

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Once we have decided to attack a particular server, our next step is to gather information about its operating system and available services. This is often referred to as "OS fingerprinting" or "application fingerprinting," because by examining small details we can often identify the software and version quite specifically. 

Our tools will fall into two general categories: active and passive. Active tools generate traffic to elicit a response from the server or network. Passive tools, on the other hand, simply capture and analyze traffic without sending any data. The upside of passive tools is that they are difficult if not impossible to detect; however, they require that the attacker has the ability to sniff the target's traffic. Active tools are more aggressive and can be detected, but they can be used to gain more information and are effective even if the attacker cannot sniff the target's traffic.

---

<!-- Slide 48 -->
<!--[Rectangle 2]-->
Nmap Port Scanner

<!--[Rectangle 3]-->
- The most popular port scanner is Nmap by Fyodor
- Actively scans a target and reports open ports
- Can detect the OS and service versions
	- -O option invokes OS fingerprinting
	- -sV option invokes service version detection
- Nmap is currently using its second generation OS fingerprinting
- Better chance of accurate detection than its First-Gen capabilities
	- Written as a Summer of Code project

Note:
<!--[Rectangle 7]-->
48

<!--[Notes Placeholder 10]-->
Nmap is an extremely popular port scanner.  It is typically used to enumerate the open ports on systems, and can also identify the operating system and listening services with a fairly high degree of accuracy.  This can allow an attacker to tailor exploits and attacks to the target system (while being very useful for network administrators and security professionals as well).

For an attacker, one of the biggest problems with Nmap is that it is easily detected.  Nmap operates by sending a packet to each port on the target host (the port range is configurable).  Based on the received responses, Nmap determines whether the port is "open", "closed", or "filtered" by a firewall.  The timing and method of Nmap's scans can be configured to lower the likelihood of detection.  Slow scans are generally less likely to be noticed, as they create less network traffic in a given period of time.  TCP SYN scans are stealthier than TCP connect scans, because TCP SYN scans send only a SYN packet and never complete a TCP connection.  Intrusion Detection Systems (IDS) detect these types of scans by default, but because port scans are so common, most people just ignore such alerts.

In addition to post scanning capabilities, Nmap is also able to perform operating system and service version detection through the "-O" and "-sV" options, respectively.  Currently leveraging second generation techniques for OS fingerprinting, Nmap has increased its accuracy over earlier first-generation deployments following significant contributions as part of a Google Summer of Code project.

Nmap is actively maintained and available at www.insecure.org.

<!-- SKIPPED: [Slide Image Placeholder 17]-->

---

<!-- Slide 49 -->
<!--[Picture 1]-->

<!--[Title 1]-->
# Nmap Example


<!--[Rounded Rectangle 6]-->

<!--[TextBox 3]-->
Nmap requires root privileges to perform operating system detection

<!--[Left Arrow 4]-->

Note:
<!--[Rectangle 7]-->
49

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
This slide demonstrates the use of Nmap to identify both the application service version information as well as the operating system of a target host.  As we can see, inguardians.com was scanned and we can see what ports are open.

---

<!-- Slide 50 -->
<!--[Title 1]-->
# Server Profiling


<!--[Content Placeholder 2]-->
- Profiling the server configurations is the next step
	- Some of the same tools can be used
- Identifying the server software and versions can help guide our attacks
	- Based on known vulnerabilities or configuration issues

Note:
<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Now we will move to the next step, profiling the server.  This is a deeper test than just detecting the OS, examining the configuration and identification of supporting infrastructure devices.

Every server is different, not only in what software is running, but also with respect to the surrounding network topology and relationships with other systems.  For each server you will want to identify the software serving HTTP, including plugins and extra features, any SSL support provided (remember, SSL is a wrapper for TCP protocols), the type of virtual server hosting the web site (IP-based or name-based virtual host), and whether the site is hosted behind a load-balancer.  

Identifying server software provides a great deal of information about the attack surface and the techniques that will be required.  If the server is Apache on FreeBSD with PHP4 installed, the attack profile will significantly differ from an IIS.NET box with FrontPage Extensions.

Versions of SSL and supported ciphers impact various forms of attack, from man-in-the-middle attacks to service-level vulnerabilities.

Understanding the type of virtual hosting used by the site is vital for knowing how to interact with the site and crafting your attack.  If name-based virtual hosting is employed by the site and your attack does not include the appropriate Host: header, the attack will not work.  SSL-support will also be limited on name-based virtual hosts, since SSL is setup by IP Address.  Other sites of interest may live on the same IP address, and information about the host and network ***may*** be obtained by poking at the Host: header or pulling the default page for the IP address.

Load balancers introduce complexity for attackers.  Most often, the important thing to understand is how the site implements persistence.  Some load balancers will tie a session to a particular server.  Multi-connection attacks are simpler for this type of persistence, while other approaches are required for other load balancing schemes.


<!-- SKIPPED: [Slide Number Placeholder 3]-->

---

<!-- Slide 51 -->
<!--[Rectangle 2]-->
Server Version

<!--[Rectangle 3]-->
- Web servers are a main target of the test
	- But not the only target. don't forget about database servers, client systems, etc.
- The server type and version significantly affects the test
	- May be vulnerable to attack
	- Different server types can impact the attack methods we'll choose
- There are multiple ways to gather server  
version information
	- A thorough tester should use multiple  
means to determine the server type and  
version for increased accuracy

<!--[Picture 3]-->

<!--[Picture 4]-->

Note:
<!--[Rectangle 7]-->
51

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
As web servers are a main target during a test, determining the version and type becomes very important.  The type will affect the test as the may be vulnerable to attack themselves.  They also change how we approach things such as injection flaws and other web attacks.  This is because the attacks use the application to attack the underlying server or operating system.

Since this is important for a test, we try to run multiple tests to verify the results.  As part of a thorough penetration test, an analyst should leverage multiple techniques to determine the server for increased accuracy since this will significantly influence the remainder of the test.

---

<!-- Slide 52 -->
<!--[Picture 1]-->

<!--[Rectangle 2]-->
Nmap

<!--[Rectangle 3]-->
- Nmap can perform version detection of the services found with the -sV option
	- Without a port designation, Nmap will scan a set of default ports
- Nmap connects to each open port and looks for a banner
	- If none is presented, Nmap sends "nudge" packets
- Nmap matches responses to signatures in an application database
	- **nmap-service-probes** contains the probes and responses expected
- New fingerprint contributions welcome!

<!--[Rounded Rectangle 6]-->

<!--[TextBox 3]-->
Identified both Apache as the HTTP server and Debian as the OS

<!--[Left Arrow 4]-->

Note:
<!--[Rectangle 7]-->
52

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Nmap is wonderful for revealing a wealth of information about target hosts including banner information which often reveals detailed information about each service. Using the "-sV" argument, Nmap will perform a service-version scan on the target host (using the "-A" argument will cause Nmap to perform both service-version scanning and OS identification).

Since the application/OS fingerprinting database is only as good as the signatures behind it, please submit new fingerprints to the Nmap developers when the opportunity arises.  Nmap will tell you when it has encountered a new service, providing the signature details and a description of how to submit the data to improve the reliability of the service version detection feature.

---

<!-- Slide 53 -->
<!--[Rectangle 2]-->
Using Netcat to Grab Server Connection Strings

<!--[Rectangle 3]-->
- Netcat: Swiss army knife of network connections
- Testers can use it to connect to a web server and retrieve pages, inspect server response data
- Header data may reveal the server's version
- However, information that comes back may be a lie
	- An administrator may have configured the system to provide a bogus server string

<!--[Rectangle 5]-->
		**printf "GET / HTTP/1.0\n\n" | nc -v www.secureideas.net 80**

Note:
<!--[Rectangle 7]-->
53

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
One of the simplest but most powerful tools is Netcat.  It is the network Swiss army knife, partly due to the fact that it can connect to any network service and allow the attacker to try and communicate with the service.

For a webapp pen-tester, Netcat can be used to connect to a web server and manually enter HTTP verbs and associated data to retrieve pages or otherwise manipulate the server into revealing useful information.  While not typically visible to a web browser, web server responses including "X-Powered-By" and "Server" are very useful to the webapp pen-tester.

While the server banner information is often accurate, it could be falsified by an administrator to mislead an attacker (for example, the website could be configured to return "Microsoft-IIS/6.0" when it is actually an Apache server).  As such, server response information should be considered suspect and evaluated for further confirmation.

---

<!-- Slide 54 -->
<!--[Title 1]-->
# Netcat Server Version


<!--[Rectangle 3]-->
$ **printf "GET / HTTP/1.0\n\n" | nc -v www.secureideas.net 80**
secureideas.net [10.42.10.42] 80 (http) open
HTTP/1.1 200 OK
Date: Fri, 16 Jan 2009 20:47:22 GMT
Server: Apache/2.2.4 (Ubuntu)
Last-Modified: Sun, 04 Jan 2009 01:45:41 GMT

<!--[Left Arrow 4]-->

<!--[Rectangle 5]-->
$ **printf "GET / HTTP/1.0\n\n" | nc -v isc.secureideas.com 80**
isc.secureideas.com [65.173.218.96] 80 (http) open
HTTP/1.1 401 Authorization Required
Date: Fri, 16 Jan 2009 20:52:27 GMT
Server: nc -l -p 80
WWW-Authenticate: Basic realm="DShieldDevSite"
X-Powered-By: ASP.NET

<!--[Rectangle 6]-->
True!

<!--[Left Arrow 7]-->

<!--[Rectangle 8]-->
Not True!

Note:
<!--[Rectangle 7]-->
54

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
The client example above is one way to connect to a web server and retrieve a fingerprint of the server.  This example redirects the printf to Netcat, which connects to the server on port 80.  The server then responds, since the printf statement is a request for the main page of the site.  As seen in the example on this slide, this technical can reveal useful version information from the web server.


---

<!-- Slide 55 -->
<!--[Rectangle 2]-->
HTTPrint

<!--[Rectangle 3]-->
- HTTPrint is a cross-platform tool to fingerprint web servers
- Most platforms support the command line version
- Windows version includes a GUI
- Uses a fingerprint database to match on how it handles the request
- Does more than just look at the "Server:" string returned in HTTP responses
- After grabbing the banner, HTTPrint runs the following tests
	- Checks the capitalization of various headers
	- Determines the header field ordering
	- Sends an improper HTTP version
	- Also sends an improper protocol

<!--[TextBox 4]-->
Different web servers respond differently to these conditions. We're fingerprinting them based on their Layer 7 behavior

<!--[Right Brace 5]-->

<!--[Picture 6]-->

Note:
<!--[Rectangle 7]-->
55

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
HTTPrint is a tool available from Net-Square.  It is very simple to use and will fingerprint a server.  You can download it from http://www.net-square.com/httprint/.  The GUI version is only available for Windows while the CLI version works on most platforms.

This is a screen shot of the HTTPrint tool where two web servers are being fingerprinted.  One returns quite a bit more information than the other.  Of course, the server admin can configure the web server to return false information.  This is why we recommend that you use multiple methods to verify the results.

One of our favorite HTTPrint features is that it can read the scan results from Nmap and just fingerprint web servers that have been discovered through port scans.  On larger networks and tests this becomes very handy as a time-saving measure.


---

<!-- Slide 56 -->
<!--[Content Placeholder 2]-->
- Webserver analysis site, reporting server use statistics for public sites
	- Also generates anti-phishing data
- Webserver version based on returned banner
	- Can be falsified by your target
- Public search interface
	- Reports history of webserver service, version
	- Only useful for publicly accessible websites
- Reveals your target (and your address) to Netcraft
	- Otherwise passive versioning technique

<!--[Title 1]-->
# Netcraft Detection


<!--[Picture 4]-->

Note:
<!--[Rectangle 7]-->
56

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Netcraft is an Internet services company providing analysis findings for the utilization of various webservers on the Internet as well as information on phishing attack sites.  By collecting information through active polling techniques and through the Netcraft Toolbar, Netcraft has collected several years of data about the distribution of webserver and version information on a large percentage of the Internet.  For some sites, Netcraft can identify the dates when a site upgraded from one webserver version to another.

While querying a webserver with Netcraft does not directly interact with the target website, it does reveal the target and your IP address to Netcraft.  Before using Netcraft, it is important to evaluate the terms of your pen-test scope and other contractual obligations to ensure this is not a contract violation.

---

<!-- Slide 57 -->
<!--[Picture 2]-->

<!--[Picture 1]-->

<!--[Title 1]-->
# Netcraft Example


<!--[Rounded Rectangle 6]-->

<!--[Rounded Rectangle 6]-->

Note:
<!--[Rectangle 7]-->
57

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
This slide illustrates an example of using Netcraft to identify the web server and OS of the site www.inguardians.com.  Additional information about the target is also available, including the netblock owner (often revealing the hosting provider's ISP information) and domain registrant information.

---

<!-- Slide 58 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
- *** Exercise: Gathering Server Info***
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
58

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
In this next exercise, we will try out and explore the various ways to gather information about the web server.

---

<!-- Slide 59 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
- *** Analyzing HTTPS Support***
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
59

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
SSL is not synonymous with "security".  It is simply a tool which is very helpful in implementing security.  The precise SSL versions and ciphers that a server supports is very important.

---

<!-- Slide 60 -->
<!--[Rectangle 2]-->
Analyzing HTTPS Support   
of Target Machines

<!--[Rectangle 3]-->
- HTTPS support is another important item to check
	- We need to make sure the set up is configured securely
	- Older versions and lower encryption levels are easier to crack and have known vulnerabilities
- We will check a variety of items
	- Does the server support HTTPS?
	- Which versions are supported?
		- SSLv2 or v3 or TLSv1
		- Anything below SSLv3 or TLSv1 is older and has issues
	- Which ciphers?  Which key lengths?
		- 128 bit? NULL cipher?
		- The NULL cipher and lower encryption levels are either weak or plain text
	- Does the application allow HTTP access to resources that should be protected by HTTPS?
	- Is the certificate expired or considered invalid by the browser?
- We will now look at various tools available to measure this info

Note:
<!--[Rectangle 7]-->
60

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
There are various tools available to assist in testing the HTTPS configuration of a web server.  Three of the more popular are OpenSSL itself, The Hacker's Choice THCSSLCheck, and SiteDigger from Foundstone.

---

<!-- Slide 61 -->
<!--[Rectangle 2]-->
Scripting  
OpenSSL

<!--[Rectangle 3]-->
- Used on the server to support and configure HTTPS
- Also able to test a server configuration
- Tools are already available on most servers
- Test for SSLv2
	$ **openssl s_client -connect www.secureideas.com:443 -ssl2**
- Test for Null Cipher
	$ **openssl s_client -connect www.secureideas.com:443 -cipher NULL**
- Very scriptable

<!--[Picture 3]-->

Note:
<!--[Rectangle 7]-->
61

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
OpenSSL allows us to generate, sign, manage and validate certificates, as well as make SSL connections directly. (Some people refer to OpenSSL as the "SSL Swiss-Army knife," in the same way that Netcat is the "TCP/IP Swiss Army knife.") This powerful tool can provide the hands-on access to SSL connections that Telnet and Netcat provide for clear-text services.  For example, if you are manually hacking HTTP or SMTP, the "OpenSSL s_client" will let you hack HTTPS and SMTPS as well.

Example:

openssl s_client -connect www.secureideas.com:443 -ssl2
openssl s_client -connect www.secureideas.com:443 -cipher NULL


On the slide, we can see two different command lines to test for various SSL configurations.  The first checks to see if the server supports the weaker version 2 of SSL.  The second verifies if the server has enabled the NULL cipher, which actually transmits the protocol in plain text.  As you can see, these types of checks can easily be wrapped into a script.

---

<!-- Slide 62 -->
<!--[Rectangle 2]-->
Using THC SSL Check  
to Evaluate Targets

<!--[Rectangle 3]-->
- THC SSL Check was released by The Hacker's Choice (THC)
	- Written by Johnny Cyberpunk
	- A Windows .EXE, but we can run in WINE on Linux
- Command line-tool that uses the negotiation of the SSL set up to determine SSL properties
	- Determines the various cipher levels and SSL version supported
- THC SSL Check connects to the service multiple times, changing its SSL and encryption level or type
	- It then displays whether the connection was successful which signifies the level is supported

Note:
<!--[Rectangle 7]-->
62

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
THCSSLCheck was written by Johnny Cyberpunk and is available from the THC site.  As shown in the screen shot, this tool is run from the command line and its results are quite extensive.  I find that taking these results and using them in scripts make this a wonderful way to test SSL.

---

<!-- Slide 63 -->
<!--[Rectangle 2]-->
Using SSLDigger  
to Evaluate Targets

<!--[Rectangle 3]-->
- Another similar tool by Foundstone
- .NET application that tests SSL versions and strength
	- SSLDigger also connects to the server multiple times
	- Each time, it attempts with different SSL versions or encryption levels
- Generates a report that includes a grade
	- Grade is the tool author's opinion
	- Tester should evaluate the supported levels and determine the risk level

<!--[TextBox 3]-->
Strength Rating

<!--[TextBox 3]-->
Checkmarks signify this is supported

<!--[Picture 1]-->

<!--[Left Arrow 4]-->

<!--[Left Arrow 4]-->

Note:
<!--[Rectangle 7]-->
63

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Foundstone has released a number of great tools.  SSLDigger is a graphical tool that checks the various ciphers and strengths offered by the server. As you can see in this screenshot, SSLDigger displays the results of each check in the top window. And in the bottom displays the progress and the current score this site is given.  This score is based on the A-D,F letter grading scale used in US schools and should be taken as a starting point.

---

<!-- Slide 64 -->
<!--[Title 1]-->
# Evaluating HTTPS Supporton Targets


<!--[Content Placeholder 2]-->
- The strengths and "correct" HTTPS support level is potentially different for each site
- The tester should report any issues found during this test
- These findings may be items such as:
	- SSL version 2 or earlier support (known weaknesses)
	- Lower levels of encryption (anything less than 128-bit)
	- Weak hashing algorithms (MD5)
- Expired, bad, or other certificate errors should also be reported
	- 

Note:
<!--[Rectangle 7]-->
64

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Now that we have collected this data about the supported HTTPS levels, we need to determine if it matches what we feel the site should support.  While different sites can support lower levels of SSL due to business requirements, we do have certain standards that the HTTPS support should meet.

For example, support for SSLv2 or earlier should be reported as there are known vulnerabilities versions of SSL before SSLv3.  Levels of encryption below 128 bits being supported are also things we would report.  Weak hashing algorithms can also be problematic, including the use of MD5 as opposed to SHA1, SHA256 and SHA512.

Of course, no matter the setup, any issues with the certificate itself, such as being expired or having a name mismatch with the site are reported as issues.

---

<!-- Slide 65 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
- *** Exercise: Testing HTTPS***
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
65

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
In this next exercise, we will test the strength of the installed SSL certificates.

---

<!-- Slide 66 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
- *** Virtual Hosting and   
   Load Balancers***
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
66

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Testing for virtual hosting is an important step in a penetration test for many reasons.  One, these other applications may be in scope and provide exploitable weaknesses.  Two, they may be out of scope and our test will need to keep in mind to stay away from them, which is harder when we start looking at command injection and other server targeting attacks.

---

<!-- Slide 67 -->
<!--[Rectangle 2]-->
Virtual Hosting

<!--[Rectangle 3]-->
- Virtual hosting is using one physical server to host multiple sites
	- Feature of the web server itself, not as a separate virtual machine
	- Server uses different IP addresses for each server or returns the correct site based on the **Host:** header value in each HTTP request
- Commonly used to better allocate resources or reduce costs
- Different sites on the same server may have vulnerabilities exposing all sites on that machine to attack
	- Ensure all virtual hosts are in scope and that you have permission to analyze them
	- Third-party hosting providers can make it difficult, but not impossible, to get permission
- Multiple methods for determining if virtual hosting is in use:
	- Multiple IP addresses point to the same host
	- Multiple names point to the same IP address 

<!--[Rectangle 32]-->

<!--[Rectangle 33]-->
Web server examines the Host: header variable to determine which site to respond with

<!--[TextBox 40]-->
Web  
Server

<!--[Object 2]-->

<!--[TextBox 40]-->
Client

<!--[Object 3]-->

<!--[TextBox 42]-->
Request for   
http://www.secureideas.com

<!--[Straight Arrow Connector 37]-->

Note:
<!--[Rectangle 7]-->
67

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Virtual hosting uses one server to host multiple sites.  While this improves resource utilization, it also adds complexity.  This can be good and bad from an attack perspective.

Understanding the type of virtual hosting used by a site is vital for understanding how to interact with the site and crafting your attack.  If name-based virtual hosting is employed by the site and your attack does not include the appropriate Host: header, the attack will not work. SSL-support will also be limited on name-based virtual hosts, since SSL is setup by IP Address.  Other sites of interest may live on the same IP address, and information about the host and network ***may*** be obtained by poking at the Host: header or pulling the default page for the IP address.

There are two types of virtual hosting.  The first kind is called IP-based Virtual Hosting, and applies a single IP address to each site.  The same hardware is used, but all queries to an IP address go to that site, regardless of header information.  

The second, more common form of virtual hosting is called Name-based Virtual Hosting, and uses one IP address for multiple websites on the same server.  As Name-based virtual hosting is a form of multiplexing, the server has to be able to identify which site gets a given request.  For this purpose the server consults the HTTP header called the "Host" header.  

Name-based virtual hosting uses fewer IP addresses to service more web sites, which is useful since the IPv4 address space is very crowded. 

We can determine if virtual hosting is in use by evaluating DNS records for the target.  If multiple IP addresses point to the same host, then virtual hosting is implemented with a different IP address for the hosts.  If multiple

---

<!-- Slide 68 -->
<!--[Title 1]-->

<!--[Content Placeholder 2]-->

Note:
<!--[Notes Placeholder 2]-->
names point to the same IP address, then the Host: header value is used to differentiate each host (note that this can also be used together on the same host, where a server hosts both IP-address virtual hosts and Host: based virtual hosts for one or more dedicated IP addresses.  This is common in hosting provider environments, where a single server is used for customers that require SSL support (thereby requiring their own IP address) and customers who wish for a less-expensive hosting option without support for SSL (Host: virtual hosting).

<!-- SKIPPED: [Slide Number Placeholder 3]-->

---

<!-- Slide 69 -->
<!--[Title 1]-->
# Virtual Hosting Detection with Bing


<!--[Content Placeholder 2]-->
- Bing offers a search modifier that shows sites on a single address
	- ip:66.135.47.101 will find the sites
- This only returns sites known to Bing
	- So if the site hasn't been indexed it won't show

<!--[Picture 1]-->

<!--[Picture 2]-->

Note:
<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Bing has a great modifier to help us find virtually hosted sites within our target.  We can use the **ip:** directive with the IP address of our target and Bing will return all the results it has for that IP address.  This allows us to find the other sites on that target.  As long as these applications are in-scope for the test, they can be leveraged to attack our target organization.

Keep in mind that Bing will only return sites it has indexed.  Even if the site is virtually hosted, if Bing hasn't indexed it or the other sites, they won't show up in the results.

<!-- SKIPPED: [Slide Number Placeholder 3]-->

---

<!-- Slide 70 -->
<!--[Rectangle 2]-->
Load Balancers

<!--[Rectangle 3]-->
- Devices to manage load by directing traffic between multiple servers
- Servers should be identical, hypothetically
	- Reality says otherwise
- Differences may be a weakness that can be attacked
- Also keep in mind where your attacks land
	- Injection attacks that allow file creation on one server do not put files on all servers in the cluster
	- This could lead to confusion or even false negatives - "I placed my file, but when I went to see it or execute it, it failed. perhaps it's not really vulnerable"
- Various methods for a penetration tester to identify load balancers
	- URL analysis
	- Timestamp analysis
	- Last modified values comparison
	- Load balancer cookies detection
	- HTTPS differences
	- HTML source code discrepancies

Note:
<!--[Rectangle 7]-->
70

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Load balancers are deployed to increase availability of a website, by splitting the workload among many servers, and keeping the web site up even if one or more servers fail.

Ideally (and probably on paper), the entire server farm is split into purpose-based clusters, each cluster made up of identical machines.  For example, the web-server cluster which provides site searching capability may be made up of three or four dedicated "search" web servers. Using identical systems in a cluster is rarely the case in real-world implementations.  Whether by lack of diligence or planning, or by design, one machine is invariably different from the others.  Often there is one server which is running the next version of operating system or web server software, to see how the site survives the change.  When Windows 2003 came out, many web farms used this approach once the site/server combination graduated from lab-testing.  Although many upper-tier web farms have developed effective patching systems, some sites still struggle to keep all their servers consistently patched.  

Most websites are written with the expectation that the users will access the same web server for the entire session.  Maintaining every session's state across every server is costly, and maintaining state in some back-end data store introduces its own performance challenges.  Locking a web browser session into a single server is called using "sticky" sessions.

Stickiness and other discrepancies introduced by the load-balancing process can help identify load-balancing in a web application.  

Be cautious about jumping to conclusions early, as some sites include multiple forms of load-balancing.  Frequently, local load-balancing (balancing between servers at the same location or region) is combined with global load-balancing (determining which global site is a better match for a user).

---

<!-- Slide 71 -->
<!--[Rectangle 2]-->
Identifying Load Balancing Implemented via URL

<!--[Rectangle 3]-->
- With load balancing based on URLs, requests are redirected to a different URL
	- www.secureideas.com redirects to www2.secureideas.com
- Most obvious method of load balancing from a test perspective
- Simplest test to perform
	- Surf to the website, and look at the difference in the URL in your browser
		- These host names can be found using the DNS lookups earlier or by using a script that iterates through the possible names
		- Fierce Scanner is an excellent tool for doing this type of test

Note:
<!--[Rectangle 7]-->
71

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
One method of handing off a session to a particular member of the server cluster is for the load balancer to redirect the browser to another domain name.  For example, you may browse to the website www.secureideas.com, but get redirected to www2.secureideas.com, or another server with a slightly different hostname.  From a test perspective, this is one of the easiest forms to find since the browser URL bar indicates which server you are actually talking to.  Simply surf to the website and look for variations in the browser URL line (clearing your stored cookies may be useful if you consistently get a single site in this approach).  Also, using the Fierce Scanner can be useful for this kind of assessment, looking for multiple DNS entries with similar hostnames (web1, web2, web3, etc.).

---

<!-- Slide 72 -->
<!--[Rectangle 2]-->
Identifying Load Balancing Using Timestamp Analysis

<!--[Rectangle 3]-->
- One of the problems with server clusters is time synchronization
- Request a page multiple times
	- Examine the **Date:** header variable for changes in time aside from normal time lapse
	- Backwards in time is a common indicator of multiple target systems

<!--[Picture 6]-->

<!--[Rounded Rectangle 6]-->

<!--[Rounded Rectangle 6]-->

<!--[Rounded Rectangle 6]-->

<!--[Rounded Rectangle 6]-->

<!--[Straight Arrow Connector 37]-->

<!--[Straight Arrow Connector 37]-->

<!--[Straight Arrow Connector 37]-->

<!--[Straight Arrow Connector 37]-->

<!--[Rectangle 24]-->

<!--[Rectangle 25]-->
The time stamp increments for the first two requests, then goes back in time

<!--[Rectangle 14]-->

Note:
<!--[Rectangle 7]-->
72

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Keeping all servers in time synchronization is harder that it sounds.  Even with NTP and Microsoft Domain Controllers as time-servers, often some servers get a little out of sync with the others.  This can help us identify load-balancing.

Request a given page multiple times; don't limit your analysis to only two queries as this may not indicate a shift in the reported clock, or you may get the same host multiple times by coincidence.  Also ensure you're not sending back any session information like cookies as this may cause your connection to get forwarded to a single server; use Netcat!

$ printf 'HEAD / HTTP/1.0\nHOST: secureideas.com\n\n' | nc -v secureideas.com 80 

If the HEAD function is not implemented or available, try:
$ printf 'GET / HTTP/1.0\nHOST: secureideas.com\n\n' | nc -v secureideas.com 80 |head

Your mileage may vary depending on the time synchronization practices of your target.  Google, for instance, tends to keep their servers in great time-sync.

---

<!-- Slide 73 -->
<!--[Rectangle 2]-->
Identifying Load Balancing Using the Last Modified Date

<!--[Rectangle 3]-->
- Request the same page
- Examine the Last-Modified header for differences in the timestamp
- This is often caused because different servers get code at different times during deployment of applications

<!--[Picture 6]-->

<!--[Rounded Rectangle 6]-->

<!--[Left Arrow 4]-->

Note:
<!--[Rectangle 7]-->
73

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
The following BASH scripting example connects to the web server and requests the default page for http://www.secureideas.com/ 100 times and prints out the Last-Modified header.  We can look for variations in the last-modified header to identify different servers in a load-balancing cluster, since not every server will receive the mirrored files at the same time.  Note that this technique is not accurate on all systems, since not every page will have this header, depending on the URL type and server configuration.

$ for (( x=1; x< 100 ; x++ )); do 
      printf 'HEAD / HTTP/1.0\nHost: www.inguardians.com\n\n' | \
      nc www.inguardians.com 80
 done |grep -i last-modified
Last-Modified: Tue, 28 Aug 2007 17:56:26 GMT
Last-Modified: Tue, 28 Aug 2007 17:56:26 GMT
Last-Modified: Tue, 28 Aug 2007 17:56:26 GMT
Last-Modified: Tue, 28 Aug 2007 17:56:26 GMT
Last-Modified: Tue, 28 Aug 2007 17:56:26 GMT
Last-Modified: Tue, 28 Aug 2007 17:56:26 GMT

---

<!-- Slide 74 -->
<!--[Rectangle 2]-->
Identifying Load Balancing Using Load Balancer Cookies

<!--[Rectangle 3]-->
- Many load balancers use a cookie
- This cookie helps determine which server to use
	- The load balancer will set a cookie in the browser recording the server the request is sent too
	- As further requests come in, the load balancer reads the cookie and directs the request to the same server
- Look for cookies referencing network infrastructure
	- Cisco CSS or Big-IP
- Don't recognize a cookie? 
	- Google for it!

Note:
<!--[Rectangle 7]-->
74

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Because many Internet users are hidden behind mega-proxies such as those from AOL, millions of users may appear to come from the same IP address.  Worse yet, because users are often load-balanced to different proxies, the source IP address may change in the middle of a session.  To keep track of actual web browser sessions, regardless of IP address, many load-balancers are configured to place a session cookie in the browser uniquely identifying that browser.  These cookies can be dead giveaways for load-balancing.

Each load-balancer will have its own default cookie names, or they may be custom cookies.  Hint: look for "LB", "Load" or "Balance" in the cookie-name.

---

<!-- Slide 75 -->
<!--[Rectangle 2]-->
Identifying Load Balancing Using HTTPS Differences

<!--[Rectangle 3]-->
- While the HTTPS configuration should match across servers, it doesn't always
- HTTPS ciphers or version support levels may be different
- HTTPS certificates may be issued to the individual servers instead of the load balanced name
	- Currently the best way to verify certificates is manually browsing the site
	- Periodically during the test, check and compare to previous checks

Note:
<!--[Rectangle 7]-->
75

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Keeping server configurations consistent is even more difficult than keeping up with patches.  Often, the HTTPS configuration will differ between servers, which can indicate that multiple servers are answering.

This method of identification is not always possible, however, since some larger sites use SSL accelerators as front-ends to their load-balancers.  For example, Cisco provides HTTPS accelerator cards for their load-balancers (known as Content Switches, or CSS).  HTTPS is a costly function, and sites get more performance from the servers by offloading the encryption onto task-specific hardware.  This approach also helps troubleshooting network traffic, as the traffic between the CSS and web servers is unencrypted.

---

<!-- Slide 76 -->
<!--[Rectangle 2]-->
Identifying Load Balancing Using HTML Source Code Differences

<!--[Rectangle 3]-->
- Some applications insert HTML comments
- Assists with troubleshooting which server is having issues
- Tag can be obvious
	- **webserver01 **or **hoc08wuw15**
- Or may be a code known to administrators
	- **<!-- AfT5T --> **or **<!-- Z4Rtg -->**

Note:
<!--[Rectangle 7]-->
76

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Another place to look for this type of information is the actual source code of the HTML sent to the browser.  We have seen many applications that insert little hidden text blocks or HTML comments that identify the server the client is using.  Typically these comments are used in testing or supporting the application. For example, if a number of users are calling the help desk and complaining about a misbehaving application, the techs can ask for this information from the comments to narrow the support effort.

---

<!-- Slide 77 -->
<!--[Title 1]-->
# Testing Web Apps onLoad-Balanced Infrastructures


<!--[Content Placeholder 2]-->
- We identify a load balancer, it changes our test approach somewhat
	- Attacks that inject code or read files should be done on all servers involved in the application
	- Differences in the application may expose an issue on only one of the servers
- The test should either take this into account or document the concern
	- Documenting the concern doesn't fix the issue

Note:
<!--[Rectangle 7]-->
77

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
If we have identified that a load balancer is being used, our test approach must change.  For examples, any attacks that inject or read files on the server should be attempted against each server in the cluster.  We need to modify the test to take this into account or document that we did not.

---

<!-- Slide 78 -->
<!--[Title 1]-->
# Test Modifications for Load Balancers


<!--[Content Placeholder 2]-->
- There are various methods for testing against load balancers
	- If accessible, address each server individually
		- www1.secureideas.com and www2.secureideas.com
		- This may require the test to be performed from within the tested network
	- Modify the load balancer to direct the tester's IP address to one server
		- Change such a config to get access to the other servers after finishing one

Note:
<!--[Rectangle 7]-->
78

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Various methods to test this set up are available to us. For example, we could access each server individually.  This only works if they are accessible via name or IP address. One thing to be careful of is the application switching us to the main URL for the load balanced version.

We could also configure the load balancer to send our source IP address to one server.  Then when we have finished the test against one, we change the configuration.

---

<!-- Slide 79 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
- *** Analyzing Software Configuration***
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
79

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
The next step is to determine the software's configuration. We are going to examine this step in the next few slides.

---

<!-- Slide 80 -->
<!--[Rectangle 2]-->
Software Configuration

<!--[Rectangle 3]-->
- Understanding the configuration is an important step
	- Configuration of the underlying server machine (OS, network services, etc.)
	- Configuration of the web server daemon itself
- What features are available?
	- Is PHP supported on the target?
	- Which HTTP request methods are accepted?
- Are there any default pages?
	- Examples or documentation

Note:
<!--[Rectangle 7]-->
80

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Once the network layout is determined and other low-impact reconnaissance has been conducted, it's time to determine what we're dealing with at a software level.  Thus far, we have been scoping the playground, and now we are about to check out the sand in the box.  Knowing what software and configurations exist in our target environment will help us determine how best to form the sand to build our empire (one sand-castle at a time).

Two of the ways to determine the configuration is to check the supported HTTP methods and look for default pages.


---

<!-- Slide 81 -->
<!--[Rectangle 2]-->
Supported HTTP Request Methods

<!--[Rectangle 3]-->
- Determine the HTTP request methods supported by the target infrastructure
- Look for interesting ones
	- PUT - WebDAV method that allows files to be PUT onto the server
	- DELETE - Allows for removing files
	- CONNECT -  Tunnel within the HTTP protocol
	- TRACE - Echo the request as seen by the server
	- OPTIONS - List supported methods
- There are many different ways to check which methods are supported by the server
- Most of the tools we use list supported HTTP methods as part of their output or report

Note:
<!--[Rectangle 7]-->
81

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Most likely, every web server will support the "GET" method of HTTP.  However, the following methods can be more interesting:

PUT - Used by WEBDAV to write files to the web server
DELETE - Used by WEBDAV to delete files from the web server
CONNECT - Creates a TCP Tunnel through the server/proxy
TRACE - Shows the request as the server received it, including modifications made by intermediary servers
OPTIONS -  Displays the supported methods

As part of the pen-test, we need to identify all available  request methods.  Fortunately, several tools are available that will help us with this task.

---

<!-- Slide 82 -->
<!--[Rectangle 2]-->
Using Netcat to Determine Supported HTTP Request Methods

<!--[Rectangle 3]-->
- Netcat is perfect for testing available HTTP request methods
- Manually type HTTP commands into Netcat or script the requests
- A bash script can iterate through request method types, invoking Netcat for each:
- 

<!--[Rectangle 5]-->
**#!/bin/bash**
**for method in GET POST PUT TRACE CONNECT OPTIONS; **
**do**
**    printf "$method / HTTP/1.1\nHost: target.tgt\n\n" | nc target.tgt 80 **
**done**

<!--[Rectangle 3]-->

Note:
<!--[Rectangle 7]-->
82

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
As discussed earlier, Netcat is simple but powerful.  Since it allows the attacker to issue command either directly to the server or through scripts, mapping out supported methods becomes simple: query the server for each potential method, as shown in the shell script on this slide.

---

<!-- Slide 83 -->
<!--[Picture 5]-->

<!--[Rectangle 2]-->
Default Pages

<!--[Rectangle 3]-->
- A common issue is default pages left on a target server
- These pages can identify the server software and lead to vulnerabilities
	- Example files and applications are often part of the default install
- Documentation is commonly left on servers
- Try accessing via IP address instead of hostname
	- This bypasses name-based virtual hosting
- Many of the tools discussed in class will discover default pages on targets
	- Nikto is a great tool for finding default content, especially associated with security issues

<!--[Picture 9]-->

<!--[Picture 4]-->

Note:
<!--[Rectangle 7]-->
83

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
The other thing to look for is default pages.  These are files installed when the server was built.  Examples of this are the welcome page from Apache or some of the sample scripts installed on IIS.

One technique for checking for default pages is to access the server via its IP address.  Many servers are configured to serve up different pages based on the fully-qualified hostname the client is requesting, but server administrators sometimes forget to configure a custom site for the IP address itself, revealing default web pages.

---

<!-- Slide 84 -->
<!--[Rectangle 2]-->
Nikto

<!--[Rectangle 3]-->
- Nikto is a perl program written by Chris Sullo
	- http://www.cirt.net
- It uses a "database" of items to scan for on the server
	- Comma separated files
	- Various files containing: 
		- Widely used server-side scripts and programs known to be vulnerable (CGI, ASP, PHP, etc.)
		- Response strings from various servers
		- MD5 hashes of favicons of specific servers
- Discovers default files residing on the server
- May have false positives due to the way servers and applications handle missing pages
	- This is one of the main reasons we need to manually verify results from scanners

<!--[Picture 3]-->

Note:
<!--[Rectangle 7]-->
84

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Nikto is a great tool for automatically looking for vulnerable web applications.  It contains a database of vulnerable applications and attempts to access each of them in turn.  In the newest version it even compares the favicon.ico file on the server to known icons to fingerprint the application.

---

<!-- Slide 85 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
- *** Exercise: Nikto***
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
85

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
In the next exercise, we will use Nikto to find default pages.

---

<!-- Slide 86 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
- *** Spidering a Target Site***
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
86

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
As part of mapping, we will now spider the site using various tools.

---

<!-- Slide 87 -->
<!--[Rectangle 2]-->
Spidering the Target Site

<!--[Rectangle 3]-->
- The next step of mapping is to spider the web application
- Spidering involves following web links to download a copy of an entire site
- We can then analyze offline to find:
	- Potential security weaknesses in code
	- E-mail addresses, names, phone numbers, etc.
	- List of keywords for password-guessing attacks
	- Confidential data
	- . and more
- Also known as "crawling" a site

Note:
<!--[Rectangle 7]-->
87

<!--[Text Box 3]-->
"Spidering" (or "crawling") a web site is when an attacker follows the links on a web site and downloads each page encountered. The result is that the attacker has a copy of the entire publicly-linked site. 

The attacker can then analyze the site offline at his or her leisure, browsing the source and identifying potential security weaknesses without creating additional server web logs. Dictionary creation tools will parse the downloaded web site and produce a list of keywords, which can then be used in password-guessing attacks.  The attacker also can scrape e-mail addresses, names, phone numbers and other data from the downloaded site for use as targets in brute-force or social engineering attacks. Finally, the attacker can conduct an exhaustive search for confidential data in the site (passwords in the source, or sensitive information accidentally made public), without tipping off the victim.

Keep in mind that we will be spidering the site MANY times during our test.  Most of our tools for discovery will need to know that site, but there is no standard storage for results.  This means that each tool will need to spider the target.

<!--[Rectangle 4]-->

---

<!-- Slide 88 -->
<!--[Rectangle 2]-->
Spidering Methods

<!--[Rectangle 3]-->
- Spidering a site can take many forms
	- It is common to spider a site multiple times since most tools need a map of the site to start scanning
	- Each tool stores this map in an internal form, not usually accessible to other tools
- Spidering can be automated or manual
	- Manual spidering is a fancy way to say "browse the site and save each page"
	- May be necessary if automated scanning fails
	- Automated scans may fail because the site is complex or has issues with multiple simultaneous requests

Note:
<!--[Rectangle 7]-->
88

<!--[Text Box 3]-->
Spidering a site is a major part of any web penetration test.  As a matter of fact, it is quite common to repeatedly spider a site.  This is because of the tools we use requiring a site map, and not being able to read the results from a different tool.  (This is something that we should all be working to fix!)

We can manually "spider" a site or use automated tools.  The manual method is basically browsing to each page and selecting the File -> Save as option.

<!--[Rectangle 4]-->

---

<!-- Slide 89 -->
<!--[Rectangle 2]-->
Robot Control

<!--[Rectangle 3]-->
- Automated spidering tools are commonly referred to as robots or bots
- One method of controlling this type of robot is a robots.txt file
	- Placed in the document root of the web application, readable by anyone accessing the website
	- Specifies which User-agent types should be disallowed access to certain directories or individual pages
	- This is not a security control, contrary to popular belief
- Alternatively, "Robots" meta tags can be used on individual pages
- Tags to prevent caching of the content
	- **<META HTTP-EQUIV="PRAGMA" CONTENT="NO-CACHE">**
	- **<META HTTP-EQUIV="CACHE-CONTROL" CONTENT="NO-CACHE">**
	- These first two should both be used as different clients respect each of them****
- Tags to control search engine spiders and where they go
	- **<META NAME="ROBOTS" CONTENT="INDEX,NOFOLLOW"> **
	- **<META NAME="GOOGLEBOT" CONTENT="NOARCHIVE">**
	- These two are useful for controlling search engine bots

<!--[Picture 6]-->

Note:
<!--[Rectangle 7]-->
89

<!--[Text Box 3]-->
Automated spidering tools are known as "robots."  These are often used by search engines, such as Google, to generate a database of pages available on the Web. Robots can also be used by web site administrators to check the validity of links, by spammers to gather e-mail addresses, or for other purposes.

The Robots Exclusion Protocol is an unofficial, commonly used standard for allowing web site administrators to specify areas which robots should not crawl. The administrator simply creates a file called "robots.txt" in the root directory of their web site, and lists in it directories and pages which should not be crawled. Many robots, including the Google crawler, check for this file and act accordingly. However, robots can ignore this file, and it can also be helpful for attackers, since it provides a convenient list of pages and directories that the administrator does not wish to be widely accessed.

Web site administrators can also use the HTML "Robots" META tag to indicate that robots should not view a particular page. However, this method is not in common use. 

Robots Exclusion Protocol:
http://www.robotstxt.org

<!--[Rectangle 4]-->

---

<!-- Slide 90 -->
<!--[Rectangle 2]-->
Automated Spidering with ZAP (1)

<!--[Rectangle 3]-->
- ZAP is an interception proxy from OWASP
	- It includes one of the better spidering capabilities
- Spider is primed by using the interception proxy
	- Browse to the starting point.
	- In the spider tab, select the request and invoke spidering
- ZAP spiders most web sites quite well
	- Client-side dynamically generated links can cause it to miss pages
		- For example, consider this JavaScript:  **var link = "http://" + $hostname + "/index.php";**
		- Spiders such as Burp and tools such as Ratproxy are better able to handle these types of sites

<!--[Picture 3]-->

Note:
<!--[Rectangle 7]-->
90

<!--[Text Box 3]-->
ZAP, the Zed Attack Proxy, is a general-purpose web application testing tool. It can be used to spider a web site. ZAP will gather a list of links, and fetch them on command. In this example, we will spider the web site hosted at 127.0.0.1:80. 

First, make sure your browser is configured to use a local proxy. Then, open ZAP, and click on the "Spider" tab.  Next, open Firefox, and enter "127.0.0.1:80" into the browser bar. Hit "Enter" to download the page. You will see the address for http://127.0.0.1:80 listed in ZAP.

<!--[Rectangle 4]-->

---

<!-- Slide 91 -->
<!--[Picture 1]-->

<!--[Rectangle 2]-->
Automated Spidering with ZAP (2)

<!--[Rectangle 3]-->
- ZAP provides status of the spidering with a progress bar
- Handles most applications
- Will list out of scope links
	- Helps us map additional targets

<!--[TextBox 3]-->
Status regarding links spidered and links queued

<!--[Left Arrow 4]-->

Note:
<!--[Rectangle 7]-->
91

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
ZAP, the Zed Attack Proxy, from OWASP is a great tool to use for spidering. It allows us to spider the site while providing some feedback on how long it will take.  It also includes a listing of out of scope targets it encountered during the spidering.  This allows us to see if additional targets may exist.

---

<!-- Slide 92 -->
<!--[Title 1]-->
# Automated Spidering with the Burp Suite


<!--[Content Placeholder 2]-->
- Burp Suite is a collection of tools for web penetration testing
	- Includes spidering capability
- Free basic version, commercial version with advanced features
	- We will cover this tomorrow
- Using the spider is similar to Paros and ZAP
	- Use a browser pointed at Burp as an interception proxy
	- Surf to the page we wish to start spidering from
	- Switching to the spider tab in Burp, select "Spider running" to start

<!--[Picture 3]-->

Note:
<!--[Rectangle 7]-->
92

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Burp suite is a collection of tools that includes a pretty advanced web spider. (We will cover the rest tomorrow.) You can download Burp from portswigger.net.  Using the spider is very similar to ZAP and Paros.  We point our browser to the proxy port of Burp and browse to the starting point we want to launch the spider from.  We the select the spider tab within Burp and start it running.

Burp Suite is available from portswigger.net.

---

<!-- Slide 93 -->
<!--[Rectangle 2]-->
Automated Spidering with Wget

<!--[Rectangle 3]-->
- Wget is a console-based web browser
- Runs on most platforms and has basic   spidering capabilities
- Wget will save each of the items retrieved
	- It saves them in a directory named after the website
- Since it is a "well-behaved" spider,   wget first downloads robots.txt and adheres to its directives
	- To ignore robots.txt, invoke with the **-e robots=off **option
- The **-****r** option invokes it to recurse through discovered links
- The **-l [N] **to specify maximum link recursion depth (default is 5)

<!--[Picture 1]-->

<!--[Rounded Rectangle 6]-->

Note:
<!--[Rectangle 7]-->
93

<!--[Text Box 3]-->
Wget is a utility distributed as part of the GNU free software project. It is packaged with most modern Linux operating systems, and has also been ported to Windows. 

The primary purpose of wget is simply to retrieve content from web sites, via HTTP, HTTPS and FTP. Its popularity arises from the fact that it can be run non-interactively from the command line, meaning that it can be included in scripts or easily configured to run periodically without human intervention. (It's also free, which helps.)?

Wget can function as a simple spider, using the "--recursive" option. When using the "-r" option, wget will extract links from pages and download each linked page, recursively repeating this process until all resources have been downloaded or the maximum recursion depth set by the user has been reached.  

Wget includes many helpful options. For more information, see:

GNU Wget
http://www.gnu.org/software/wget/

<!--[Rectangle 4]-->

---

<!-- Slide 94 -->
<!--[Title 1]-->
# Specialized Spidering Tools


<!--[Content Placeholder 2]-->
- There also a number of tools that perform a spider and look for specific data
- One of these is CeWL from Robin "digininja" Wood
	- Custom Word List Generator
	- http://www.digininja.org
- CeWL spiders a web site and then generates a word list for use as passwords or other dictionaries for attacks
	- Will grab information from EXIF data

Note:
<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
One of the other types of tools we may use during a penetration test are specialized spiders.  These spiders will scan a web site and return specific data that we need.  For example, there are ones that will return all of the comments in a site or grab e-mails exposed.

One of the most commonly used ones is CeWL by Robin Wood.  CeWL, the Custom Word List Generator, spiders a web site and generates a word list based on both the contents of the site and EXIF data from any images found.  We can use these lists as passwords for any brute-force attacks we want to perform.

<!-- SKIPPED: [Slide Number Placeholder 3]-->

---

<!-- Slide 95 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
- *** Exercise: Web Spidering with   
    wget, ZAP, and Burp***
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
95

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
In this exercise we will use the various web spiders and examine how they work.

---

<!-- Slide 96 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
- *** Analyzing Spidering Results***
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
96

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
What do we look for during spidering is explored in this next section.

---

<!-- Slide 97 -->
<!--[Rectangle 2]-->
Analyzing Spidering Results:   
What to Look For

<!--[Rectangle 3]-->
- Once we have spidered a site, we analyze the results to look for various items, including:
	- Comments that reveal useful or sensitive information
	- Commented code and links
	- Disabled functionality
	- Linked servers
		- Such as content and application servers

Note:
<!--[Rectangle 7]-->
97

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Once you spider the site you can look for various interesting tidbits.  Some examples would be comments in the HTML, functionality that has been disabled and servers that are linked to the application.

---

<!-- Slide 98 -->
<!--[Rectangle 2]-->
HTML Comments

<!--[Rectangle 3]-->
- Web applications commonly contain comments in the HTML they send to the browser
- Such comments are included in the server response and visible by the client
	- Developer notes can reveal significant issues
		**"<!-- Don't forget to fix the auth bug ****-->"******
	- Explanations of functionality
		**"//This function passes input to the query****"******
	- Explanations of variables
		**"/* Variable for user auth level. don't change! *****/"******
	- Usernames and passwords
		**"<!- The db user/pass is scott/tiger ****-->"**
- Such comments should be moved to server side comments
	- For example, PHP would use **<?php //Don't forget to fix the auth bug ?>**
	- Of course they could just fix the bug. that's a much better approach!

Note:
<!--[Rectangle 7]-->
98

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Comments can be very revealing.  They can include everything from usernames and passwords to obscenities regarding management and particular languages.  At the very least, comments can provide information that will help you launch social engineering attacks.  In many cases they can also show you how the application works under the hood, or point you at parts of the application that have problems.  For some reason developers often forget that the end user can just click "View Source"!

---

<!-- Slide 99 -->
<!--[Rectangle 2]-->
Disabled Functionality

<!--[Rectangle 3]-->
- As web applications evolve, some developers leave behind disabled functionality
- Disabled functionality often reveals previous or future sections of the site
	- These need to be tested and reported on
	- While officially "disabled", sometimes such functionality can be invoked
	- It may contain significant security weaknesses, as it often gets less attention than other components of a site or application
	- Even though "disabled", such functionality could lead to undermining the entire application

Note:
<!--[Rectangle 7]-->
99

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
As web applications grow and expand, some developers will leave disabled functionality within the application.  We may be able to find previous versions of the application or sections within it.  Or we may get a sneak peek into new areas being built.

---

<!-- Slide 100 -->
<!--[Rectangle 2]-->
Types of Disabled Functionality

<!--[Rectangle 3]-->
- Links that have been commented out
	- These links could link to older, future or privileged pages
	- Most of the spidering tools will list comments found
- Client-side code that has been commented out
	- This code could show how server side-code works
	- Functionality that was replaced by server-side code
	- Again the tools we have discussed will list comments and script code

Note:
<!--[Rectangle 7]-->
100

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
The first two items might appear to fall under the previous section of this course, that is, finding comments in HTML sent to the client.  However, since these relate directly to functionality that appears to be either disabled or not working, they should be analyzed differently than the other types of comments.

It is very common to find links that have been commented out so they do not appear on the page or menu.  As an attacker I focus on these as most of the time they provide links to things the application owner doesn't mean for you to see.

Older hidden pages contain functionality that hasn't been kept up to date.  Since the most important part of session, authentication and authorization is consistency, finding older pages that may not be as secure can enable us to abuse the application.

Newer hidden pages are commonly found shortly before a site is updated.  Developers will start uploading code in preparation and by finding these pages, we have a chance of finding code that is not complete or is still being tested. It is common to have the test version of pages display troubleshooting information including queries being used.  We can use this information to launch attacks.

Privileged pages are portions of the site that we are not authorized to access.  Some applications verify the user's level of authorization, and then display a menu. If you view the source, sometimes you can see links to administrative pages which are commented out since the user does not have access. By browsing to the pages, you may find that the pages themselves do not verify your authorization level.  Brilliant!

---

<!-- Slide 101 -->
<!--[Rectangle 2]-->

<!--[Rectangle 3]-->

Note:
<!--[Rectangle 3]-->
Code that has been commented out usually falls into one of two categories:

	- The first type is code that has been replaced by server side code. By being able to see the client-side version, the attacker may be able to identify weaknesses in the code. 
	- The second type is code that is no longer used.  This code may point at either functions or areas of the site that are no longer used.  These sections of the site may be a weakness that  can be exploited.

These functions are interesting because of a couple reasons.  First, they may show a weakness that wasn't obvious before.  And second, sometimes client side functions are moved server side and then just dead ended in the client code.  This may give you a view into how the server side code works.


<!-- SKIPPED: [Slide Number Placeholder 3]-->

---

<!-- Slide 102 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
- *** Application Flow Charting***
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
102

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Application flow is the next part of mapping.

---

<!-- Slide 103 -->
<!--[Rectangle 2]-->
Charting the Application Flow

<!--[Rectangle 3]-->
- Pen testers need to understand and document an application's logic flow between pages and functions
- Flow charting enables an attacker to see the path through an app graphically 
- Two different flows should be charted
	- Normal Usage and Abusive Usage
- Charting methods:
	- Simple pen and paper
	- Diagramming software
	- Spidering software

<!--[Diagram 4]-->

Note:
<!--[Rectangle 7]-->
103

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Up until now, we've focused on identifying components of the application. In the next step, we focus on gaining a better understanding of how the site functions as a whole. Application flow charting will allow us to visualize the program flow.  Being able to view the program flow on paper (or computer screen) allows us to better understand where the weak points and obvious trust relationships are, how the application components interact, and identify the accessible attack surface.  

Keep in mind that we are interested in both normal and abnormal interactions with the site. 

Charting is not magic.  It can be done using pen and paper. Freeform diagrams are very easy to implement using a pen.  Abstract concepts are simple to place, as are last-minute changes in understanding.  

Charting and diagramming software such as Visio can be very helpful if you need to be able to update the document over time and possibly share it with others on  your team.  However, this software is not always intuitive and takes more effort than the pen and paper method.

Spidering software can also generate charts and lists to chart an application.

---

<!-- Slide 104 -->
<!--[Rectangle 2]-->
App Flow Charting with  
Pen and Paper

<!--[Rectangle 3]-->
- Most familiar method
- Sketch out a diagram listing pages
- As we move from page to page, connect them with a line
	- Multiple lines will connect a page to all of the others
- Notes for each page
	- Interesting parameters
	- Forms or interesting content
- Pros: 
	- Inexpensive, quick and easy
- Cons: 
	- Sharing scribbles can be a problem, not directly usable in reports

<!--[Picture 4]-->

<!--[Picture 5]-->

Note:
<!--[Rectangle 7]-->
104

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Pen and paper is one of the first communication mediums we learn, and they are  by far the most common. There is something about pen and paper which allows us to abstractly lay out information freely and without detracting from the creative side of our brains.  Common computer interfaces have yet to achieve anything close to this simplicity.  Even with some of the complex tools we use during attacks, it surprises me how often we fall back to just jotting a diagram on the back of a piece of paper.


---

<!-- Slide 105 -->
<!--[Rectangle 2]-->
App Flow Charting with Diagramming Software

<!--[Rectangle 3]-->
- Using various graphing tools, we can build the site map
	- Examples include Visio, Kivio, and Omnigraffle
- Note each page and the links between pages representing application logic flow
- The one major benefit is the amount of data per page that can be stored
	- Limited only by your hard drive

<!--[Picture 4]-->

<!--[TextBox 3]-->
As of Visio 2003, web mapping is a feature

<!--[Left Arrow 4]-->

<!--[TextBox 3]-->
Visio icons with connectors

<!--[Left Arrow 4]-->

Note:
<!--[Rectangle 7]-->
105

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Many programs have been created to create complex diagrams in a fashion which  is easily updated, shared, and backed-up.  The most common are Visio for Windows, Kivio (from the KDE project) for Linux, and Omnigraffle for Mac OSX.  There are countless other tools for visualizing complex systems.

---

<!-- Slide 106 -->
<!--[Rectangle 2]-->
App Flow Charting with  
Spidering Software

<!--[Rectangle 3]-->
- Some spidering tools generate their output of spidered links in a graphical format
	- If not, most will at least record and display lists of inter-related links
- Use the results to identify relationships

<!--[Picture 1]-->

<!--[TextBox 3]-->
Sitemap within the Burp Suite

<!--[Left Arrow 4]-->

Note:
<!--[Rectangle 7]-->
106

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Some spidering software, such as ZAP and the Burp suite, includes relationship-graphing  functionality built in.  Even those which don't provide this functionality will output a list of links for each page, which can be used to build relationships  manually or with a script.  Your favorite spider may require a little scripting on  your part to get valuable relationship data.

---

<!-- Slide 107 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Client-Side Discovery
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	- *** ***<span style="text-decoration: underline;">***Relationship Analysis***</span>
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Creating Custom Scripts for   
     Penetration Testing
	-  Python for Penetration Testing
	-  Exercise: Python Scripting 
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	- *** ***<span style="text-decoration: underline;">***Relationship Analysis***</span>
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
107

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
In this section we will be exploring the relationships between the pieces and parts of the application.

---

<!-- Slide 108 -->
<!--[Rectangle 2]-->
Relationship Analysis

<!--[Rectangle 3]-->
- Now that we have a site map, we need to analyze the results
- We are looking for relationships between the various parts of the application
	- Pages, sections, code styles, naming systems
- We need to find areas of the site we have missed
- We can also start looking for potential weaknesses 
	- Authorization bypass
	- Issues with filtering not correctly preventing attacks

Note:
<!--[Rectangle 7]-->
108

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
A great wealth of information can be obtained through understanding the relationship of various components of the web application.  Page relationships and section relationships can help identify direct attack points.  Code styles may provide insight into the way the developer thinks, and can identify where different developers coded different portions of the site.  

---

<!-- Slide 109 -->
<!--[Rectangle 2]-->
Relationship Analysis:  
Dividing the App into Sections

<!--[Rectangle 3]-->
- Now that we have looked for missing pages, we need to determine which sections of the application exist
- Similar to pages as to what we are looking for
- Sections are groupings of related pages
- Identify how pages are organized
- Common section break downs may be
	- "About the company" pages
	- Search pages
	- Catalog pages
	- User management pages
	- Order processing
- Finding these sections allows us to focus on what we are looking for
	- "About the company" pages reveal data for password guessing
	- Catalog pages may be a focus of SQL injection
	- User management would be a place to search for authorization bypass

Note:
<!--[Rectangle 7]-->
109

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Sections of a site are typically categories of pages that relate.  For example, a site may be broken up in sections such as:
- News
- Products
- Support
- Admin

By identifying these sections, the attacker can divide the work between areas of the site.  Commonly, these areas may contain specific functions for each section.

---

<!-- Slide 110 -->
<!--[Rectangle 2]-->
Relationship Analysis:  
Looking for Missed Pages

<!--[Rectangle 3]-->
- One of the first items to look for are pages that have been missed
	- Pages that should be there.
	- .but we've (so far) found no links to them or they have been missed by our spidering tools
- Look for items that should be there based on the application's functionality
	- Online bank: Check balances, transfer funds, manage accounts, report problems, get help
	- Social Network: Profile, friend management, external applications, messaging 
- As an example, if you have found pages such as adduser.php, viewuser.php and deluser.php, check for edituser.php as this is a common function that fits the others

Note:
<!--[Rectangle 7]-->
110

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
By building this analysis, we are able to find pages that were missed.  These pages could be missed due to them not being linked, errors in our configuration of our spidering or technology problems.

We should do this by looking at the functionality and finding features that are missing.  For example, social networks have functions related to profiles, friend or connection management and messaging.  If we find that our map does not include connection management, we need to determine if it was missed.  Of course it might just not be in the application. ?

---

<!-- Slide 111 -->
<!--[Rectangle 2]-->
Relationship Analysis:   
Identifying Code Styles

<!--[Rectangle 3]-->
- Now we should look at client-side code
	- Analyze both the HTML and scripts sent back to the browser
- Look at the coding style to find weaknesses and vulnerabilities
- Also look for signs of different developers
	- Different code styles also show that the site may not be following an organizational standard
	- Difference could include variable naming schemes, function naming schemes, tendency to split code into blocks, use of comments, use of whitespace

<!--[Straight Arrow Connector 4]-->

<!--[TextBox 3]-->
As we can see the code on the left uses generated code while on the right we have what appears to be manually generated code

<!--[Picture 1]-->

<!--[Straight Arrow Connector 4]-->

<!--[Picture 2]-->

Note:
<!--[Rectangle 7]-->
111

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Programming is a creative process, and each developer has their own style of writing code.  Identifying differences in style can provide valuable insight into  how the application was developed, and where responsibilities for the code change.  How is that valuable?  Just as the physical world shows weaknesses  where different components are joined, software-development tends to be weak around the joints.

---

<!-- Slide 112 -->
<!--[Rectangle 2]-->
Different Developers Could Lead to Security Flaws

<!--[Rectangle 3]-->
- Where different developers write code,  misunderstandings can abound
- Misunderstandings can often be  leveraged for attack
	- These assumptions can be exploited
	- For example, one developer may assume that the previous part of the site validated authorization, leaving another part of the site completely exposed
	- Or, one developer may write code that blindly accepts a session token from the browser, assuming that it was set by a separate component of the application developed by someone else
	- Or, one developer may perform input filtering to block XSS while another may only do output encoding

Note:
<!--[Rectangle 7]-->
112

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
If you can tell the difference between two developers' styles, there are likely different approaches to programming interfaces, variables, etc.  It may take some creativity, but these differences can sometimes be leveraged in our favor.  For instance, both developers may expect the other one to validate input.

If you are unable to discern different coding styles, yet you know that the application was written by different developers (e.g. the comments indicate as much), the application may adhere to some corporate standard for development.  This may indicate a higher level of security, or specific, standardized security weaknesses.  If the code does adhere to a corporate standard, then when you identify one type of security vulnerability, you are likely to see it throughout the application and possibly the entire corporate environment.



---

<!-- Slide 113 -->
<!--[Rectangle 2]-->
Identifying Naming Systems Used in a Web Application

<!--[Rectangle 3]-->
- Naming systems are standards the developers have decided on, or are imposed by their development environment
- Pages, applet class files, include files, and configurations are all items discoverable with this understanding
- The naming systems assist us in finding parts of the site we are missing
- For example, there are many development frameworks
- These frameworks use a naming structure for pages and actions
- An example of this is the Fusebox framework
	- Pages that display information start with "dsp" in the page name
	- Queries to the datastore happen in pages that begin with "qry"
	- Pages that allow a user to perform an action begin with "act"

Note:
<!--[Rectangle 7]-->
113

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
When developing a larger application, agreeing upon a naming system can keep different developers (or even the same developer) from getting confused by obscure and less-than-obvious names.  Naming systems are often created to this end.  Pages, scripts, and Java servlet classes take on a specific format (e.g.  "/reports/071224-business-meeting.html").  Understanding the naming conventions limits the number of guesses attackers require to find hidden  pages and utilities.  Much of hacking is leveraging well-known assumptions,  whether to exploit the users themselves or to guess the names of hidden  components.  For instance, SQL-Injection requires that you know the name of the  table.  If we know what database engine is used we can determine the names of  tables, etc., but much time can be saved if we are interested in the users table and  it is named "tbl_users".

Likewise, function names and variable names often follow a naming convention.   In certain circumstances this information can also be leveraged, particularly in some forms of script-injection.

---

<!-- Slide 114 -->
<!--[Rectangle 2]-->
OWASP DirBuster

<!--[Rectangle 3]-->
- DirBuster is cross-platform Java application from OWASP
- Attempts to retrieve pages based on a list of terms
	- Sends HTTP requests with guesses of directory and/or file names
	- Includes word lists that were built by large-scale crawling of the Internet to identify common directory and file names
	- Other tools' word lists or custom lists can be used too
	- It uses the extensions configured by the tester
- It also spiders any files found, grabbing a copy for off-line analysis
- While many testers write a script to handle such searches, the multithreaded nature of DirBuster ranks it as one of the better tools

<!--[Picture 1]-->

Note:
<!--[Rectangle 7]-->
114

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
OWASP DirBuster is a Java application that is part of the OWASP project.  Since it is written in Java, it can run on most platforms.  It can run in two different modes.  The first uses a wordlist, while the second actually generates strings to try.  DirBuster ships with a number of lists.  These lists were generated by crawling the web and then sorted by number of times a specific word was found.  The lists are also available for user name enumeration with the Apache UserDirectory module.

These lists are available as a separate download for use in other scripts.

The screen above and to the left if the first screen within DirBuster.  It is where you can configure DirBuster to find web directories.  The second screen shot is DirBuster in action.  You can see the item found in the center.  To the lower right, DirBuster displays the current directory being looked for.

---

<!-- Slide 115 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	- *** Exercise: OWASP DirBuster***
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
115

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
In this exercise, we will use DirBuster to find pages and directories within our target.

---

<!-- Slide 116 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
- *** Session Analysis***
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
116

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Session gathering and analysis is the final step in the mapping phase.

---

<!-- Slide 117 -->
<!--[Rectangle 2]-->
Session Token Gathering

<!--[Rectangle 3]-->
- Most web applications create sessions to track a user through a series of interactions
- As we explore a site to map it, the application creates sessions for us
- These sessions typically have a session identifier variable, sometimes called a session token or session credential, passed to the browser
	- We should collect them to determine if they are predictable 
	- If they are, an attacker may be able to determine other users' session credentials and usurp their accounts
- Various ways to collect them
	- Manually
	- Customized scripts
	- Burp Suite

Note:
<!--[Rectangle 7]-->
117

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Maintaining the concept of a "session" across many different TCP connections is commonly accomplished using any combination of cookies,  URL-parameters, hidden-form-fields, and IP/browser information.  If we can understand the logic behind maintaining session state, we may be able to manipulate it to attack the application.

The simplest example of a session token is a cookie called USERID.  If you look at your cookie USERID and it says your name in Fullname.Lastname notation, you may very well be able to become the user "Kevin.Johnson" by  modifying that cookie to be "KEVIN.JOHNSON".  Session token manipulation is based on the concept of manipulating "hidden" parts of your web browsing session and observing how it affects the application.  If you can isolate the things which make up a session, either you can modify your session parameters at will (including, in some cases, whether you have administrative access) or you know what to grab in your XSS/CSRF attacks.

What about a USERID cookie with the value of" S2V2aW4gSm9obnNvbg=="?   How about "72b28b1b696ecaadfc0f212f16809850", or "655609cdf4d93495c9f3166e6d"?  In order, those were Base64 encoded, md5sum, and hex-encoded output from crypt with the password of "abcdefg."  All represent opportunities to manipulate the application with new session tokens of the attacker's choosing.

Sending repeated, "new" (without session information)  requests for the login page using tools like Wget, Python or Netcat will normally cause the web application to generate a new session ID each time.  Look for patterns in the generated session IDs.  Are they somewhat sequential?  Do they change in an identifiable pattern?  Are there any fields which do not change?  Sometimes a timestamp is encoded or encrypted, and occasionally seconds may be omitted, resulting in encrypted, encoded, or hashed values that remain the same for the whole minute.

---

<!-- Slide 118 -->
<!--[Title 1]-->
# Session Token Variables


<!--[Content Placeholder 2]-->
- Applications pass session tokens  
back to browsers using different mechanisms
	- URL parameters passed via HTTP GET
	- Hidden form elements passed via   
HTTP POST
	- Cookies
- Some applications use multiple  
means.
	- Either on separate pages  
in the app
	- Or, even on the same page

<!--[Picture 6]-->

<!--[Picture 8]-->

<!--[Picture 9]-->

<!--[TextBox 6]-->
URL-Based Session

<!--[TextBox 6]-->
Hidden Form Field-Based Session

<!--[TextBox 6]-->
Cookie-Based Session

Note:
<!--[Rectangle 7]-->
118

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Web applications often need to track session state across requests using various techniques.  They may pass data via the URL as shown at the top with the jsessionid.  They may also use hidden form fields as in the middle graphic where the ViewState variable is set as a hidden form input.  The final method is using a cookie.  At the bottom we have a Set-Cookie response header creating a JSESSIONID cookie.

Keep in mind that some applications are complex and may use multiple methods.

---

<!-- Slide 119 -->
<!--[Title 1]-->
# Identifying Session Tokens


<!--[Content Placeholder 2]-->
- Identifying standard session credentials created by application development environment
	- Some automated tools do this for you, such as WebScarab (more on that shortly)
	- Java and JSESSIONID or PHP and PHPSESSIONID
	- Determining session credentials based on their names
		- Google is your friend!  Research variable names via web searches
		- Several examples include items such as session, sessionid, or sid
	- Finding session credentials based on its behavior and intuition
		- Observing a variable that changes for each login
		- If we remove a cookie, does the site prompt us to log in again?

Note:
<!--[Rectangle 7]-->
119

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
One of the steps we must do is to identify the method of passing session state.  Some of the tools we use, such as WebScarab and w3af, will attempt to determine this for us.  But they do not always succeed.  We need to look for known session identifiers and if they are not found, we need to decide which variables are being used.  Look for variables that change for each login or block a variable and see if we get redirected to the log in page.

---

<!-- Slide 120 -->
<!--[Title 1]-->
# Session Token Predictability


<!--[Content Placeholder 2]-->
- Session tokens may be predictable
- Consider incremental tokens
	- First login: 74eb2cd93f2a95ba
	- Next login: 74eb2cd93f2a95bb
	- Next login: 74eb2cd93f2a95bc
	- Next login: 74eb2cd93f2a95bf
- Realize that other users may access a production site while you are sampling and you may not get the entire series - significant gaps may appear
- Other predictable assignments:
	- Change by fixed constant (42, 84, 126, 168, etc.)
	- Or, consider:
	- c4ca4238a0b923820dcc509a6f75849b, c81e728d9d4c2f636f067f89cc14862c, eccbc87e4b5ce2fe28308fd9f2a7baf3, a87ff679a2f3e71d9181a67b7542122c
	- Other possible patterns include: 
	- IDs based on client IP addresses or data from the session encoded
	- 
	- 
	- 

<!--[Freeform 3]-->

<!--[TextBox 4]-->
Why the gap?  See next bullet!

<!--[Right Brace 5]-->

<!--[TextBox 6]-->
Can you name the algorithm?  If not, see notes.

Note:
<!--[Rectangle 7]-->
120

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Session token predictability is a serious problem if it exists within the target application.  If the token can be predicted, an attacker could guess what session tokens have already been used and then hijack an active session.

In this slide we show some examples of predictable tokens.  We have incremental tokens in the first pattern.  The second pattern of hex digits on the screen is the md5 hash of the ascii representation of integers starting at 1.  We see md5sum(1), md5sum(2), md5sum(3), and md5sum(4).

We also find that if the developer creates their own session token, we will often find that they use client data such as IP address as the token.  They typically encode or hash it in some manner but we should be able to detect that in our testing.

---

<!-- Slide 121 -->
<!--[Rectangle 2]-->
Manually Collecting Session Credentials

<!--[Rectangle 3]-->
- While this method is not used often, sometimes it is required
	- This may be because the application is complex and requires human interaction or has issues with automated tools
- As you browse the site record the session credential
	- Flat file or spreadsheet works well
- Analysis can then be done using Excel or Calc

Note:
<!--[Rectangle 7]-->
121

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
A not very useful method, would be to manually record the session token as you browse the site.  While it may seem like a waste to mention this, I have tested sites that for various reasons, could not withstand a scripted tool and I needed to manually record the various session tokens I came across.  This is not very common and is usually a problem the application owner needs to fix.  But keep this in mind if you ever come across one of these types of applications.

---

<!-- Slide 122 -->
<!--[Rectangle 2]-->
Collecting Session Credentials via Customized Scripts

<!--[Rectangle 3]-->
- Another place to keep in mind are scripts
- As we discussed earlier, many times we write a custom script to perform some action repeatedly or iteratively
- Add a portion to log the session tokens found
- This log can then be analyzed

<!--[Picture 4]-->

<!--[Rounded Rectangle 7]-->

Note:
<!--[Rectangle 7]-->
122

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
A second method would be to add the function to any script the attacker is using to write the session token to a file.  That file can then be read into spreadsheet tools such as Excel to generate charts and graphs like the ones from WebScarab.

---

<!-- Slide 123 -->
<!--[Title 1]-->
# Burp SequencerSession Analysis


<!--[Content Placeholder 2]-->
- Burp Suite also contains a   
sequencer to analyze   
session tokens
- Similar to WebScarab, we   
seed it URLs by proxying
- The sequencer runs many   
different tests
	- It provides easy to understand descriptions of the tests
- We can also load a file of tokens to analyze
	- This allows us to analyze any data that is supposedly random 

<!--[Picture 1]-->

Note:
<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
Burp Suite also contains a session token analyzer.  It is called Sequencer.  We are able to seed it the requests we need analyzed by using the Burp Proxy.  Once we browse to the page that creates the session token, we are able to right-click on it in the Target tab.  We then select "send to sequencer".  In the sequencer tab, we are able identify the token, either by allowing Burp to determine it, or by manually selecting it.  (This manual selection is one of the benefits over WebScarab)  We are also able to load tokens from a file.  This allows us to take any type of token and determine its randomness.  For example, if the target site allows for digital downloads and randomizes the file name, we would be able to take samples of the file names and load them from a file.  One of the other nice features of the Sequencer is the number of tests it runs.  It does an excellent job of explaining what the tests are doing including explanations of the math for us mere mortals.

<!-- SKIPPED: [Slide Number Placeholder 3]-->

---

<!-- Slide 124 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	- *** Exercise: Session Analysis***

Note:
<!--[Rectangle 7]-->
124

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
In this exercise we will use WebScarab to gather session IDs and graph them.

---

<!-- Slide 125 -->
<!--[Rectangle 1041]-->
Course Roadmap

<!--[Rectangle 1042]-->
- Attacker's View,   
Pen-Testing & Scoping
- <span style="text-decoration: underline;">***Recon & Mapping***</span>
- Application Discovery
- Application Discovery Cont.
- Exploitation
- Capture the Flag

<!--[Freeform 1044]-->

<!--[Rectangle 1043]-->
- *** ***<span style="text-decoration: underline;">***Reconnaissance***</span>
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing SSL Support
-  Exercise: Testing SSL
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
	-  Linked Servers
	-  Exercise: Sniffing
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

<!--[Rectangle 1043]-->
-  Reconnaissance
-  Whois and DNS
		-  Exercise: DNS Harvesting
-  External Information Sources
-  Mapping
-  Port Scanning, OS Finger-  
     printing, & Version Scanning
-  Exercise: Gathering Server Info
-  Analyzing HTTPS Support
-  Exercise: Testing HTTPS
-  Virtual Hosting and Load Balancers
-  Analyzing Software Configuration
-  Exercise: Nikto
-  Spidering a Target Site
-  Exercise: Web Spidering with   
    wget, ZAP, and Burp
-  Analyzing Spidering Results
-  Application Flow Charting
	-  Relationship Analysis
	-  Exercise: OWASP DirBuster
-  Session Analysis
	-  Exercise: Session Analysis

Note:
<!--[Rectangle 7]-->
125

<!-- SKIPPED: [Slide Image Placeholder 1]-->

<!--[Notes Placeholder 2]-->
And now to wrap up.

---

<!-- Slide 126 -->
<!--[Rectangle 2]-->
Summary

<!--[Rectangle 3]-->
- Today we have covered the first two steps of the web app pen test methodology
- Reconnaissance
	- Searched for information
	- Used DNS to further our understanding of the target
- Mapping
	- Port scanned the network
	- Determined the OS and server types
	- Mapped the application
	- Found relationships and paths through the application
	- Finally, we analyzed session tokens for predictability
- In Section PEWAPT101.3, we will start the discovery step
	- Focusing on application problems

Note:
<!--[Rectangle 7]-->
126

<!--[Rectangle 2]-->

<!--[Rectangle 3]-->
Today we have covered quite a bit of ground.  We have run through the reconnaissance phase in an attack.  During it, we have discovered the various parts that make up the application and profiled the server and its various pieces of software.

Next we moved into mapping of the application.  We have spidered the site and discovered a wealth of information within the comments and application flow.  We have used this information to analysis the application and how it fits together.

Tomorrow we will start our discovery phase.  This is the first "malicious" traffic we will send to the server.  But every bit of it and the next stage, exploitation, depends on the information we have covered today.

Thank you for your attention.  We will embark on Section PEWAPT101.3 next.

---

